{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RoSys - The Robot System","text":"<p>RoSys provides an easy-to-use robot system. Its purpose is similar to ROS. But RoSys is fully based on modern web technologies and focusses on mobile robotics.</p> <p>The full documentation is available at rosys.io.</p>"},{"location":"#principles","title":"Principles","text":""},{"location":"#all-python","title":"All Python","text":"<p>Python is great to write business logic. Computation-heavy tasks are wrapped in processes, accessed through WebSockets or called via C++ bindings. Like you would do in any other Python program.</p>"},{"location":"#modularity","title":"Modularity","text":"<p>You can structure your code as you please. RoSys provides its magic without assuming a specific file structure, configuration files or enforced naming.</p>"},{"location":"#event-loop","title":"Event Loop","text":"<p>Thanks to asyncio you can write your business logic without locks and mutexes. The execution is parallel but not concurrent which makes it easier to read, write and debug. In real-case scenarios this is also much faster than ROS. Its multiprocessing architecture requires too much inter-process communication.</p>"},{"location":"#web-ui","title":"Web UI","text":"<p>Most machines need some kind of human interaction. RoSys is built from the ground up to make sure your robot can be operated fully off the grid with any web browser. This is done by incorporating NiceGUI, a wonderful all-Python UI web framework. It is also possible to proxy the user interface through a gateway for remote operation.</p>"},{"location":"#simulation","title":"Simulation","text":"<p>Robot hardware is often slower than your own computer. To rapidly test out new behavior and algorithms, RoSys provides a simulation mode. Here, all hardware is mocked and can even be manipulated to test wheel blockages and similar.</p>"},{"location":"#testing","title":"Testing","text":"<p>You can use pytest to write high-level integration tests. It is based on the above-described simulation mode and accelerates the robot's time for super fast execution.</p>"},{"location":"#architecture-and-features","title":"Architecture and Features","text":""},{"location":"#modules","title":"Modules","text":"<p>RoSys modules are just Python modules which encapsulate certain functionality. They can hold their own state, register lifecycle hooks, run methods repeatedly and subscribe to or raise events. Modules can depend on other modules which is mostly implemented by passing them into the constructor.</p>"},{"location":"#lifecycle-hooks-and-loops","title":"Lifecycle Hooks and Loops","text":"<p>Modules can register functions via <code>rosys.on_startup</code> or <code>rosys.on_shutdown</code> as well as repeatedly with a given interval with <code>rosys.on_repeat</code>.</p> <p>Note</p> <p>Note that NiceGUI's <code>app</code> object also provides methods <code>app.on_startup</code> and <code>app.on_shutdown</code>, but it is recommended to use RoSys' counterparts: <code>rosys.on_startup</code> ensures the callback is executed after persistent modules have been loaded from storage. If you, e.g., set the <code>rosys.config.simulation_speed</code> programmatically via <code>app.on_startup()</code> instead of <code>rosys.on_startup</code>, the change is overwritten by RoSys' <code>persistence.restore()</code>.</p>"},{"location":"#events","title":"Events","text":"<p>Modules can provide events to allow connecting otherwise separated modules of the system. For example, one module might read sensor data and raise an event <code>NEW_SENSOR_DATA</code>, without knowing of any consumers. Another module can register on <code>NEW_SENSOR_DATA</code> and act accordingly when being called.</p>"},{"location":"#automations","title":"Automations","text":"<p>RoSys provides an <code>Automator</code> module for running \"automations\". Automations are coroutines that can not only be started and stopped, but also paused and resumed, e.g. using <code>AutomationControls</code>. Have a look at our Click-and-drive example.</p>"},{"location":"#persistence","title":"Persistence","text":"<p>Modules can register backup and restore methods to read and write their state to disk.</p>"},{"location":"#time","title":"Time","text":"<p>RoSys uses its own time which is accessible through <code>rosys.time</code>. This way the time can advance much faster in simulation and tests if no CPU-intensive operation is performed. To delay the execution of a coroutine, you should invoke <code>await rosys.sleep(seconds: float)</code>. This creates a delay until the provided amount of RoSys time has elapsed.</p>"},{"location":"#threading-and-multiprocessing","title":"Threading and Multiprocessing","text":"<p>RoSys makes extensive use of async/await to achieve parallelism without threading or multiprocessing. But not every piece of code you want to integrate is offering an asyncio interface. Therefore RoSys provides two handy wrappers:</p> <p>IO-bound: If you need to read from an external device or use a non-async HTTP library like requests, you should wrap the code in a function and await it with <code>await rosys.run.io_bound(...)</code>.</p> <p>CPU-bound: If you need to do some heavy computation and want to spawn another process, you should wrap the code in a function and await it with <code>await rosys.run.cpu_bound(...)</code>.</p>"},{"location":"#safety","title":"Safety","text":"<p>Python (and Linux) is fast enough for most high-level logic, but has no realtime guarantees. Safety-relevant behavior should therefore be put on a suitable microcontroller. It governs the hardware of the robot and must be able to perform safety actions like triggering emergency hold etc.</p> <p>We suggest to use an industrial PC with an integrated controller like the Zauberzeug Robot Brain. It provides a Linux system to run RoSys, offers AI acceleration via NVidia Jetson, two integrated ESP32 microcontrollers and six I/O sockets with up to 24 GPIOs for digital I/Os, CAN, RS485, SPI, I2C, etc. It also has two hardware ENABLE switches and one which is controllable via software.</p> <p>To have flexible configuration for the microcontroller we created another open source project called Lizard. It is a domain-specific language interpreted by the microcontroller which enables you to write reactive hardware behavior without recompiling and flashing.</p>"},{"location":"#user-interface","title":"User Interface","text":"<p>RoSys builds upon the open source project NiceGUI and offers many robot-related UI elements. NiceGUI is a high-level UI framework for the web. This means you can write all UI code in Python and the state is automatically reflected in the browser through WebSockets. See any of our examples.</p> <p>RoSys can also be used with other user interfaces or interaction models if required, for example a completely app-based control through Bluetooth Low Energy with Flutter.</p>"},{"location":"#notifications","title":"Notifications","text":"<p>Modules can notify the user through <code>rosys.notify('message to the user')</code>. When using NiceGUI, the notifications will show as snackbar messages. The history of notifications is stored in the list <code>rosys.notifications</code>.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Thank you for your interest in contributing to RoSys! We are thrilled to have you on board and appreciate your efforts to make this project even better.</p> <p>As a growing open-source project, we understand that it takes a community effort to achieve our goals. That's why we welcome all kinds of contributions, no matter how small or big they are. Whether it's adding new features, fixing bugs, improving documentation, or suggesting new ideas, we believe that every contribution counts and adds value to our project.</p> <p>We have provided a detailed guide on how to contribute to RoSys in our CONTRIBUTING.md file. We encourage you to read it carefully before making any contributions to ensure that your work aligns with the project's goals and standards.</p> <p>If you have any questions or need help with anything, please don't hesitate to reach out to us. We are always here to support and guide you through the contribution process.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#on-your-computer","title":"On Your Computer","text":"<pre><code>python3 -m pip install rosys\n</code></pre> <p>See Getting Started for what to do next.</p> <p>Please note that RoSys has been developed for Unix-like systems. While it may partially work on Windows, we do not officially support it.</p>"},{"location":"installation/#on-the-robot","title":"On The Robot","text":"<p>While the above-mentioned installation command works perfectly well in local environments, on a robot it is often easier to run RoSys inside a Docker container. If you already have a <code>main.py</code>, it can be as simple as running</p> <pre><code>docker run -it --rm -v `pwd`:/app zauberzeug/rosys\n</code></pre> <p>from the same directory. See Pushing Code to Robot on how to get your project onto the remote system.</p> <p>More complex Docker setups benefit from a compose file. There are also some specialties needed to start RoSys in different environments (Mac, Linux, NVidia Jetson, ...). To simplify the usage we suggest to use a script called <code>./docker.sh</code> which you can also copy and adapt in your own project. Have a look at the project examples to see how a setup of your own repository may look like.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#asyncio-warning","title":"Asyncio Warning","text":"<p>While running RoSys you may see warnings similar to this one:</p> <pre><code>2021-10-31 15:08:04.040 [WARNING] asyncio: Executing &lt;Task pending name='Task-255' coro=&lt;handle_event() running at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:344&gt; wait_for=&lt;_GatheringFuture pending cb=[&lt;TaskWakeupMethWrapper object at 0x7f7001f8e0&gt;()] created at /usr/local/lib/python3.9/asyncio/tasks.py:705&gt; created at /usr/local/lib/python3.9/site-packages/justpy/justpy.py:261&gt; took 0.238 seconds\n</code></pre> <p>This means some coroutine is clogging the event loop for too long. In the above example it is a whopping 238 ms in which no other actor can do anything. This is an eternity when machine communication is expected to happen about every 10 ms. The warning also provides a (not so readable) hint where the time is consumed.</p> <p>The example above is one of the more frequent scenarios. It means some code inside a user interaction event handler (e.g. <code>handle_event()</code> in <code>justpy.py</code>) is blocking. Try to figure out which UI event code is responsible by commenting out parts of your logic and try to reproduce the warning systematically.</p>"},{"location":"development/","title":"Development","text":""},{"location":"development/#pushing-code-to-the-robot","title":"Pushing Code to the Robot","text":"<p>To get the code onto the robot you can simply pull your repository. But this requires you to have login credentials on an external machine. And editing files must be done on slow hardware compared to development workstations and laptops. If you use VS Code Remote Development or similar to do actual development on these slow systems, everything feels like jelly. Especially if you run powerful extensions like Pylance.</p> <p>That is why we at Zauberzeug created a small open source tool called LiveSync. It combines a local filesystem watcher with rsync to copy changes to a (slow) remote target whenever your local code changes. This approach has multiple advantages:</p> <ul> <li>work with your personal choice of IDE and tooling</li> <li>run tests (or simulate the production code) locally</li> <li>continuously deploy the development code to the target environment (where auto-reload ensures live preview)</li> <li>almost no overhead on the (slow) target</li> </ul>"},{"location":"development/#logging","title":"Logging","text":"<p>RoSys uses the Python logging package with namespaced loggers. For example, the steerer module writes its logs as <code>rosys.steerer</code>. This can be used for fine-granular control of what should show on the console. As a general starting point we suggest reading the Python Logging HOWTO. In the following examples we use Python's logging <code>dictConfig</code> for configuration, because it provides the most flexibility while having all configuration in one place.</p>"},{"location":"development/#show-info-messages","title":"Show Info Messages","text":"<p>To only print RoSys messages at the info level to the console we can use a configuration like this:</p> <pre><code>#!/usr/bin/env python3\nimport logging\nimport logging.config\n\nfrom nicegui import ui\n\nfrom rosys.driving import Odometer, Steerer, joystick\nfrom rosys.hardware import RobotSimulation, WheelsSimulation\n\nlogging.config.dictConfig({\n    'version': 1,\n    'disable_existing_loggers': True,  # to make sure this config is used\n    'formatters': {\n        'default': {\n            'format': '%(asctime)s - %(levelname)s - %(message)s',\n            'datefmt': '%Y-%m-%d %H:%M:%S',\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'default',\n            'level': 'DEBUG',\n            'stream': 'ext://sys.stdout'\n        },\n    },\n    'loggers': {\n        '': {  # this root logger is used for everything without a specific logger\n            'handlers': ['console'],\n            'level': 'WARN',\n            'propagate': False,\n        },\n        'rosys': {\n            'handlers': ['console'],\n            'level': 'INFO',\n            'propagate': False,\n        },\n    },\n})\n\nwheels = WheelsSimulation()\nsteerer = Steerer(wheels)\nodometer = Odometer(wheels)\nrobot = RobotSimulation([wheels])\n\njoystick(steerer)\n\nui.run(title='RoSys')\n</code></pre> <p>As you move the joystick, <code>rosys.steerer</code> messages will appear on the console:</p> <pre><code>2022-01-11 06:53:21 - INFO - start steering\n2022-01-11 06:53:22 - INFO - stop steering\n2022-01-11 06:53:23 - INFO - start steering\n2022-01-11 06:53:23 - INFO - stop steering\n</code></pre>"},{"location":"development/#adding-loggers","title":"Adding Loggers","text":"<p>You can easily add more loggers. For example, to see debug messages of the odometer you can add</p> <pre><code>'rosys.odometer': {\n    'handlers': ['console'],\n    'level': 'DEBUG',\n    'propagate': False,\n},\n</code></pre> <p>Most of the time we turn off log propagation to ensure the configuration we defined ourselves is really used.</p>"},{"location":"development/#logging-to-file","title":"Logging to File","text":"<p>Sometimes it is helpful to write intensive logging into a file and only show some messages on the console. For this you can add a file <code>handler</code>:</p> <pre><code>    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'default',\n            'level': 'DEBUG',\n            'stream': 'ext://sys.stdout'\n        },\n        'file': {\n            'level': 'DEBUG',\n            'class': 'logging.handlers.RotatingFileHandler',\n            'formatter': 'default',\n            'filename': PATH / 'example.log',\n            'maxBytes': 1024 * 1000,\n            'backupCount': 3\n        }\n    },\n</code></pre> <p>Then you can decide for each logger which handlers should be used:</p> <pre><code>    },\n    'loggers': {\n        '': {  # this root logger is used for everything without a specific logger\n            'handlers': ['console', 'file'],\n            'level': 'WARN',\n            'propagate': False,\n        },\n        'rosys': {\n            'handlers': ['console', 'file'],\n            'level': 'INFO',\n            'propagate': False,\n        },\n        'rosys.event': {\n            'handlers': ['file'],\n            'level': 'DEBUG',\n            'propagate': False,\n        },\n        'rosys.core': {\n            'handlers': ['file'],\n            'level': 'DEBUG',\n            'propagate': False,\n        },\n</code></pre> <p>Note</p> <p>The above file logger writes to <code>~/.rosys</code>. For development it is very helpful to have auto-reloading on file change activated. Therefore logging should always be stored outside of your project's source directory.</p>"},{"location":"development/#formatting","title":"Formatting","text":"<p>It is quite useful to see from which file and line number a log entry was triggered. To keep the log lines from getting too long, you can create a log filter which computes the relative path:</p> <pre><code>class PackagePathFilter(logging.Filter):\n    \"\"\"Provides relative path for log formatter.\n\n    Original code borrowed from https://stackoverflow.com/a/52582536/3419103\n    \"\"\"\n\n    def filter(self, record: logging.LogRecord) -&gt; bool:\n        pathname = record.pathname\n        record.relative_path = None\n        abs_sys_paths = map(cast(Callable[[str], str], os.path.abspath), sys.path)\n        for path in sorted(abs_sys_paths, key=len, reverse=True):  # longer paths first\n            path_ = path if path.endswith(os.sep) else path + os.sep\n            if pathname.startswith(path_):\n                record.relative_path = os.path.relpath(pathname, path_)\n                break\n        return True\n</code></pre> <p>You need to register the filter and apply it in the handler. Then you can change the format for the formatter:</p> <pre><code>    'filters': {\n        'package_path_filter': {\n            '()': PackagePathFilter,\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'filters': ['package_path_filter'],\n            'formatter': 'default',\n            'level': 'DEBUG',\n            'stream': 'ext://sys.stdout'\n        },\n    },\n    'loggers': {\n        '': {  # this root logger is used for everything without a specific logger\n            'handlers': ['console'],\n            'level': 'WARN',\n            'propagate': False,\n        },\n        'rosys': {\n            'handlers': ['console'],\n            'level': 'INFO',\n            'propagate': False,\n        },\n    },\n</code></pre> <p>Log output then looks like this:</p> <pre><code>2022-01-11 06:51:00.319 [DEBUG] rosys/runtime.py:78: startup completed\n</code></pre>"},{"location":"development/#profiling","title":"Profiling","text":"<p>Note</p> <p>The default RoSys installation via pip does not come with profiling packages. To install them, run</p> <pre><code>python3 -m pip install rosys[profiling]\n</code></pre> <p>Currently this does not work with Python 3.11 because yappy and line-profiler do not support 3.11 yet.</p> <p>You can add a <code>profile</code> decorator to expensive functions and add a profiler button to your UI:</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nimport rosys\nfrom rosys.analysis import profile_button, profiling\n\n\n@profiling.profile\ndef compute() -&gt; None:\n    s = 0\n    for i in range(1_000_000):\n        s += i**2\n    ui.notify(s)\n\n\nrosys.on_repeat(compute, 1.0)\nprofile_button()\n\nui.run()\n</code></pre> <p>When the button is pressed, the profiler yappi will start recording data. When stopped, you will see its output on the console:</p> <pre><code>Line #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     7                                           @profiling.profile\n     8                                           def compute() -&gt; None:\n     9         3         21.0      7.0      0.0      s = 0\n    10   3000003     433138.0      0.1     28.2      for i in range(1_000_000):\n    11   3000000    1098975.0      0.4     71.6          s += i**2\n    12         3       2151.0    717.0      0.1      ui.notify(s)\n</code></pre>"},{"location":"development/#track-async-function-calls","title":"Track async function calls","text":"<p>RoSys provides a <code>@track</code> decorator that you can put above asynchronous functions that are called as part of automations. The UI element <code>track.ui()</code> will show the stack of functions that are currently awaited.</p> <pre><code>#!/usr/bin/env python3\nimport asyncio\n\nfrom nicegui import ui\n\nfrom rosys.analysis import track\n\n\n@track\nasync def do_A():\n    await asyncio.sleep(1)\n\n\n@track\nasync def do_B():\n    await asyncio.sleep(1)\n\n\n@track\nasync def do_something():\n    await asyncio.sleep(1)\n    for _ in range(3):\n        await do_A()\n        await do_B()\n\nui.button('Do something', on_click=do_something)\n\ntrack.ui()\n\nui.run()\n</code></pre>"},{"location":"development/#continuous-build","title":"Continuous Build","text":"<p>We run our continuous integration with GitHub Actions. For each commit mypy and pylint scan the codebase and the pytests are executed.</p>"},{"location":"development/#releases","title":"Releases","text":"<p>We publish releases using tags and milestones on GitHub. In the Release notes we describe our changes. To create a new release perform the following steps:</p> <ol> <li><code>./fetch_milestone.py {0.x.y}</code> with the current milestone name that is to be published.</li> <li>Edit the text the script produces with more details and more mentions of people that participated.</li> <li>In your local repo add a new tag with <code>v0.{x.y}</code> as the name to the current main head.</li> <li>Push to GitHub which starts GitHub Action that performs the following steps:</li> <li>If the pytests are successful, a poetry build and deployment to pypi is issued.</li> <li>A multi-arch Docker image is built and pushed to Docker Hub.</li> <li>Close the milestone on GitHub.</li> <li>Create a new milestone on GitHub with the next version (<code>0.{x.y+1}</code>).</li> <li>Wait for the GitHub Action that was started with your push to finish.</li> <li>Edit the draft of Release notes with the text you created at step 2.</li> </ol>"},{"location":"examples/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Areas</li> <li>Camera Arm</li> <li>Cameras</li> <li>Click-And-Drive</li> <li>Coordinate-Frames</li> <li>Hardware</li> <li>Hello Bot</li> <li>Kpi</li> <li>Navigation</li> <li>Obstacles</li> <li>Path Planning Analysis</li> <li>Persistence</li> <li>Play-Pause-Stop</li> <li>Schedule</li> <li>Simulation-Speed</li> <li>Steering</li> </ul>"},{"location":"examples/areas/","title":"Areas","text":"<p>This example demonstrates how to create and modify areas that can be used for path planning.</p> <p>The example uses the <code>PathPlanner</code> to create a path planner instance. The <code>AreaManipulation</code> class is used to create and modify areas.</p> <p>The UI allows selecting the area type and color. Additionally, the UI shows a 3D representation of the area that can be used to create and modify the area.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nfrom rosys.geometry import Prism\nfrom rosys.pathplanning import AreaManipulation, PathPlanner, area_object\n\n# setup\npath_planner = PathPlanner(Prism.default_robot_shape())\narea_manipulation = AreaManipulation(path_planner)\n\n# ui\nwith ui.card().classes('mx-auto'):\n    with ui.row().classes('items-center'):\n        select = ui.select({None: 'Asphalt', 'sand': 'Sand'}) \\\n            .bind_value(area_manipulation, 'area_type').classes('w-20')\n        ui.color_input(preview=True) \\\n            .bind_value(area_manipulation, 'area_color').classes('w-40')\n        area_manipulation.create_ui()\n    with ui.scene(640, 480,\n                  on_click=area_manipulation.handle_click,\n                  on_drag_end=area_manipulation.handle_drag_end,\n                  drag_constraints='z = 0'):\n        area_object(path_planner, area_manipulation)\n\n# start\nui.run(title='areas')\n</code></pre>"},{"location":"examples/camera_arm/","title":"Camera Arm","text":"<p>This example demonstrates how to use 3D coordinate frames to model a camera arm.</p> <pre><code>#!/usr/bin/env python3\nimport math\nfrom typing import Any\n\nfrom nicegui import ui\n\nfrom rosys import config, persistence\nfrom rosys.driving import Driver, Odometer, Steerer, joystick, keyboard_control\nfrom rosys.geometry import Frame3d, Pose3d, Rotation, axes_object\nfrom rosys.hardware import RobotSimulation, WheelsSimulation\n\nwheels = WheelsSimulation()\nrobot = RobotSimulation([wheels])\nsteerer = Steerer(wheels)\nodometer = Odometer(wheels)\ndriver = Driver(wheels, odometer)\n\n\nclass Link(persistence.Persistable):\n\n    def __init__(self, name: str, parent_frame: Frame3d, *, length: float) -&gt; None:\n        super().__init__()\n        self.name = name\n        self.length = length\n        self.base = Pose3d().in_frame(parent_frame).as_frame(f'{name}_base')\n        self.end = Pose3d(z=length).as_frame(f'{name}_end').in_frame(self.base)\n\n    def pitch(self, angle: float) -&gt; None:\n        self.base.rotation = Rotation.from_euler(0, angle, 0)\n        self.request_backup()\n\n    def backup_to_dict(self) -&gt; dict[str, Any]:\n        return {\n            'name': self.name,\n            'base': persistence.to_dict(self.base),\n        }\n\n    def restore_from_dict(self, data: dict[str, Any]) -&gt; None:\n        self.name = data['name']\n        self.base = persistence.from_dict(Frame3d, data['base'])\n        self.end = Pose3d(z=self.length) \\\n            .as_frame(f'{self.name}_end') \\\n            .in_frame(self.base)\n\n\nclass Cam(persistence.Persistable):\n\n    def __init__(self, parent_frame: Frame3d) -&gt; None:\n        super().__init__()\n        self.pose = Pose3d(z=0.05).in_frame(parent_frame)\n\n    def pitch(self, angle: float) -&gt; None:\n        self.pose.rotation = Rotation.from_euler(0, angle, 0)\n        self.request_backup()\n\n    def backup_to_dict(self) -&gt; dict[str, Any]:\n        return {'pose': persistence.to_dict(self.pose)}\n\n    def restore_from_dict(self, data: dict[str, Any]) -&gt; None:\n        self.pose = persistence.from_dict(Frame3d, data['pose'])\n\n\nanchor_frame = Pose3d(z=0.3).as_frame('anchor').in_frame(odometer.prediction_frame)\narm1 = Link('arm1', anchor_frame, length=0.3).persistent(key='arm1')\narm2 = Link('arm2', arm1.end, length=0.3).persistent(key='arm2')\ncam = Cam(arm2.end).persistent(key='cam')\n\n\n@ui.page('/')\ndef page():\n    def update_scene():\n        chassis.move(*odometer.prediction_frame.resolve().translation)\n        chassis.rotate_R(odometer.prediction_frame.resolve().rotation.R)\n        segment1.move(*arm1.base.resolve().translation)\n        segment1.rotate_R(arm1.base.resolve().rotation.R)\n        segment2.move(*arm2.base.resolve().translation)\n        segment2.rotate_R(arm2.base.resolve().rotation.R)\n        camera_box.move(*cam.pose.resolve().translation)\n        camera_box.rotate_R(cam.pose.resolve().rotation.R)\n    ui.timer(config.ui_update_interval, update_scene)\n\n    keyboard_control(steerer)\n    with ui.row():\n        with ui.scene() as scene:\n            with scene.group() as chassis:\n                scene.box(width=1.0, height=0.5, depth=0.3) \\\n                    .move(z=0.15).material(color='gray')\n            with scene.group() as segment1:\n                scene.box(width=0.1, height=0.1, depth=arm1.length) \\\n                    .move(z=arm1.length / 2)\n            with scene.group() as segment2:\n                scene.box(width=0.1, height=0.1, depth=arm2.length) \\\n                    .move(z=arm2.length / 2)\n            with scene.group() as camera_box:\n                scene.box(width=0.1, height=0.1, depth=0.1) \\\n                    .material(color='SteelBlue')\n            scene.move_camera(y=-1, z=1, look_at_z=0.5)\n        with ui.column():\n            joystick(steerer, size=50, color='blue')\n            ui.slider(min=-math.pi / 2, max=math.pi / 2, step=0.01, value=0,\n                      on_change=lambda e: arm1.pitch(e.value))\n            ui.slider(min=-math.pi / 2, max=math.pi / 2, step=0.01, value=0,\n                      on_change=lambda e: arm2.pitch(e.value))\n            ui.slider(min=-math.pi / 4, max=math.pi / 4, step=0.01, value=0,\n                      on_change=lambda e: cam.pitch(e.value))\n        with ui.scene() as scene2:\n            axes_object(anchor_frame, length=0.15)\n            axes_object(arm1.base, name='Arm 1 Base', length=0.15)\n            axes_object(arm2.base, name='Arm 2 Base', length=0.15)\n            axes_object(cam.pose, name='Camera', length=0.15)\n            scene2.move_camera(y=-1, z=1, look_at_z=0.5)\n\n\nui.run(title='Camera Arm')\n</code></pre>"},{"location":"examples/cameras/","title":"Cameras","text":"<p>RoSys provides instant camera access for object detection, remote operation and similar use cases.</p>"},{"location":"examples/cameras/#setup","title":"Setup","text":"<p>USB camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program <code>v4l2ctl</code> and openCV (including python bindings) must be available. We recommend to use the RoSys Docker image which provides the full required software stack. Make sure the container can access the USB devices by starting it with <code>--privileged</code> or explicitly passing the specific <code>--device</code>s.</p>"},{"location":"examples/cameras/#show-captured-images","title":"Show Captured Images","text":"<p>Using <code>rosys.ui</code> you can show the latest captured images from each camera:</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nfrom rosys.vision import SimulatedCamera\n\ncamera = SimulatedCamera(id='test_cam', width=800, height=600)\n\nimage = ui.interactive_image()\nui.timer(0.3, lambda: image.set_source(camera.get_latest_image_url()))\n\nui.run(title='RoSys')\n</code></pre> <p>The <code>ui.timer</code> regularly updates the source property of the <code>ui.image</code>. The cameras <code>latest_image_uri</code> property provides the URI to the latest captured image.</p> <p>This example uses a <code>SimulatedCamera</code> for demonstration. You can directly replace the camera with a <code>UsbCamera</code> or <code>RtspCamera</code> if you know their ids or use their respective providers to discover them automatically.</p>"},{"location":"examples/cameras/#remote-operation","title":"Remote Operation","text":"<p>A fairly often required use case on real mobile robots is the remote operation. In a simple use case you may only need to visualize one camera and have some steering controls. Here we use the <code>NEW_CAMERA</code> event to display the first camera to control real Hardware:</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nimport rosys\n\nif rosys.hardware.SerialCommunication.is_possible():\n    communication = rosys.hardware.SerialCommunication()\n    robot_brain = rosys.hardware.RobotBrain(communication)\n    can = rosys.hardware.CanHardware(robot_brain)\n    wheels = rosys.hardware.WheelsHardware(robot_brain, can=can)\n    robot = rosys.hardware.RobotHardware([can, wheels], robot_brain)\n    camera_provider = rosys.vision.UsbCameraProvider()\nelse:\n    wheels = rosys.hardware.WheelsSimulation()\n    robot = rosys.hardware.RobotSimulation([wheels])\n    rosys.vision.SimulatedCameraProvider.USE_PERSISTENCE = False\n    camera_provider = rosys.vision.SimulatedCameraProvider()\n    camera = rosys.vision.SimulatedCamera(id='test_cam', width=800, height=600)\n    rosys.on_startup(lambda: camera_provider.add_camera(camera))\nsteerer = rosys.driving.Steerer(wheels)\nodometer = rosys.driving.Odometer(wheels)\n\n\nasync def add_main_camera(camera: rosys.vision.Camera) -&gt; None:\n    camera_card.clear()  # remove \"seeking camera\" label\n    with camera_card:\n        main_cam = ui.interactive_image()\n        ui.timer(0.1, lambda: main_cam.set_source(camera.get_latest_image_url()))\n\ncamera_provider.CAMERA_ADDED.register_ui(add_main_camera)\n\nwith ui.card().tight().style('width:30em') as camera_card:\n    ui.label('seeking main camera').classes('m-8 text-center')\n\nwith ui.card().tight().style('width:30em'):\n    with ui.row():\n        with ui.card().tight():\n            rosys.driving.joystick(steerer)\n            rosys.driving.keyboard_control(steerer)\n        ui.markdown('steer with joystick on the left or&lt;br /&gt;SHIFT + arrow keys') \\\n            .classes('m-8 text-center')\n\nui.run(title='RoSys')\n</code></pre> <p>By adding a <code>Joystick</code> and <code>KeyboardControl</code> the robot is ready to go for remote operation.</p>"},{"location":"examples/cameras/#controlling-the-camera","title":"Controlling the Camera","text":"<p>The following example creates a web interface for controlling multiple camera types. It displays cameras in a grid, showing their live feeds along with controls to connect/disconnect and adjust settings like FPS, quality, exposure, and color. The demo supports RTSP, MJPEG, USB, and simulated cameras. It automatically updates every 0.1 seconds to detect and display new cameras, and initializes with one simulated camera.</p> <pre><code>#!/usr/bin/env python3\nimport logging\n\nfrom nicegui import ui\n\nimport rosys.vision\n\n\ndef add_card(camera: rosys.vision.Camera, container: ui.element) -&gt; None:\n    uid = camera.id\n    if uid not in streams:\n        with container:\n            camera_card = ui.card().tight()\n            camera_cards[uid] = camera_card\n            print(f'adding card for {uid}')\n            with camera_grid:\n                with camera_card:\n                    streams[uid] = ui.interactive_image()\n                    ui.label(uid).classes('m-2')\n                    with ui.row():\n                        ui.button('disconnect', on_click=camera.disconnect) \\\n                            .bind_enabled_from(camera, 'is_connected')\n                    if isinstance(camera, rosys.vision.ConfigurableCamera):\n                        create_camera_settings_panel(camera)\n\n    streams[uid].set_source(camera.get_latest_image_url())\n\n\ndef create_camera_settings_panel(camera: rosys.vision.ConfigurableCamera) -&gt; None:\n    camera_parameters = camera.get_capabilities()\n    parameter_names = [parameter.name for parameter in camera_parameters]\n    with ui.expansion('Settings').classes('w-full') \\\n            .bind_enabled_from(camera, 'is_connected'):\n        if isinstance(camera, rosys.vision.RtspCamera):\n            ui.label('URL') \\\n                .bind_text_from(camera, 'url',\n                                backward=lambda x: x or 'URL not available')\n        if 'fps' in parameter_names:\n            with ui.card(), ui.row():\n                ui.label('FPS:')\n                ui.select(options=list(range(1, 31)),\n                          on_change=lambda e: camera.set_parameters({'fps': e.value})) \\\n                    .bind_value_from(camera, 'parameters',\n                                     backward=lambda params: params['fps'])\n        if 'substream' in parameter_names:\n            with ui.card():\n                ui.switch('High Quality',\n                          on_change=lambda e: camera.set_parameters({'substream': 0 if e.value else 1}))\n        if 'exposure' in parameter_names:\n            with ui.card(), ui.row():\n                ui.label('Exposure:')\n                ui.select(options=list(range(0, 255)),\n                          on_change=lambda e: camera.set_parameters({'exposure': e.value})) \\\n                    .bind_value_from(camera, 'parameters',\n                                     backward=lambda params: params['exposure'])\n        if 'auto_exposure' in parameter_names:\n            with ui.card():\n                ui.switch('Auto Exposure',\n                          on_change=lambda e: camera.set_parameters({'auto_exposure': e.value})) \\\n                    .bind_value_from(camera, 'parameters',\n                                     backward=lambda params: params['auto_exposure'])\n        if 'color' in parameter_names:\n            with ui.card(), ui.row():\n                ui.label('Color:')\n                with ui.button(icon='colorize'):\n                    ui.color_picker(on_pick=lambda e: camera.set_parameters({'color': e.color}))\n\n\ndef update_camera_cards() -&gt; None:\n    providers: list[rosys.vision.CameraProvider] = [\n        rtsp_camera_provider,\n        mjpeg_camera_provider,\n        usb_camera_provider,\n        simulated_camera_provider,\n    ]\n    for provider in providers:\n        for camera in provider.cameras.values():\n            add_card(camera, camera_grid)\n\n\nlogging.basicConfig(level=logging.INFO)\nstreams: dict[str, ui.interactive_image] = {}\ncamera_cards: dict[str, ui.card] = {}\ncamera_grid = ui.row()\n\nrtsp_camera_provider = rosys.vision.RtspCameraProvider()\nmjpeg_camera_provider = rosys.vision.MjpegCameraProvider()\nusb_camera_provider = rosys.vision.UsbCameraProvider()\nsimulated_camera_provider = rosys.vision.SimulatedCameraProvider()\n\nui.timer(0.1, update_camera_cards)\n\nsimulated_camera_provider.add_cameras(1)\n\nui.run(title='RoSys', port=8080)\n</code></pre>"},{"location":"examples/cameras/#streaming-rtsp-cameras","title":"Streaming RTSP Cameras","text":"<p>The following example shows how to stream images from an RTSP camera.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nimport rosys\n\ncamera_provider = rosys.vision.RtspCameraProvider()\n\n\ndef refresh() -&gt; None:\n    for uid, camera in camera_provider.cameras.items():\n        if uid not in streams:\n            with camera_grid:\n                with ui.card().tight():\n                    streams[uid] = ui.interactive_image()\n                    ui.label(uid).classes('m-2')\n        streams[uid].set_source(camera.get_latest_image_url())\n\n\nstreams: dict[str, ui.interactive_image] = {}\ncamera_grid = ui.row()\nui.timer(0.01, refresh)\n\nui.run(title='RoSys')\n</code></pre>"},{"location":"examples/click-and-drive/","title":"Click-and-drive","text":"<p>In this example we create a simulated robot which uses an automation to drive wherever the user clicks.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\nfrom nicegui.events import SceneClickEventArguments\n\nimport rosys\n\nwheels = rosys.hardware.WheelsSimulation()\nrobot = rosys.hardware.RobotSimulation([wheels])\nodometer = rosys.driving.Odometer(wheels)\ndriver = rosys.driving.Driver(wheels, odometer)\nautomator = rosys.automation.Automator(None, on_interrupt=wheels.stop)\n\n\nasync def handle_click(e: SceneClickEventArguments):\n    for hit in e.hits:\n        if hit.object_id == 'ground':\n            target = rosys.geometry.Point(x=hit.x, y=hit.y)\n            automator.start(driver.drive_to(target))\n\nwith ui.scene(on_click=handle_click):\n    shape = rosys.geometry.Prism.default_robot_shape()\n    rosys.driving.robot_object(shape, odometer, debug=True)\nui.label('click into the scene to drive the robot')\nwith ui.row():\n    rosys.automation.automation_controls(automator)\nui.label('you can also pause/resume or stop the running automation')\n\nui.run(title='RoSys')\n</code></pre> <p></p> Modules Besides wheels, odometer and a robot shape we need a driver that enables the robot to drive along a given path as well as an automator to start and stop such an automated behavior. Click handler NiceGUI's 3D scene allows registering a click handler that can iterate through all hit points and find the target on the ground. Driver Among others, the driver has an async method <code>drive_to</code> which lets the robot follow a straight line to a given target. Automator and automation controls The automator starts the async method and allows pausing, resuming and stopping it, e.g. with the <code>AutomationControls</code> UI element."},{"location":"examples/coordinate-frames/","title":"Coordinate Frames","text":"<p>This example shows how to use <code>Pose3d</code> and <code>Frame3d</code> to represent objects that are connected to each other. Here, we have a moving blue box and a rotating pink box with a camera attached to it. The button allows switching the pink box between the blue box's frame and the world frame at runtime. Note that the relationship between the pink box and the camera is unaffected by the frame switch.</p> <pre><code>#!/usr/bin/env python3\nimport math\n\nfrom nicegui import ui\n\nimport rosys\nfrom rosys.geometry import Pose3d, Rotation\nfrom rosys.vision import CameraSceneObject, SimulatedCalibratableCamera\n\nblue = Pose3d(z=0.5).as_frame('blue')\npink = Pose3d(z=0.75).as_frame('pink').in_frame(blue)\ncamera = SimulatedCalibratableCamera.create_calibrated(id='Camera', z=0.5,\n                                                       roll=math.pi / 2,\n                                                       frame=pink)\n\nwith ui.scene() as scene:\n    blue_box = scene.box(width=1, height=1, depth=1).material(color='SteelBlue')\n    pink_box = scene.box(width=0.5, height=0.5, depth=0.5).material(color='HotPink')\n    camera_object = CameraSceneObject(camera)\n\n\ndef update():\n    blue.x = math.cos(0.5 * rosys.time())\n    blue.y = math.sin(0.5 * rosys.time())\n    pink.rotation *= Rotation.from_euler(0, 0, 0.005)\n\n    blue_box.rotate_R(blue.resolve().rotation.R)\n    blue_box.move(*blue.resolve().translation)\n\n    pink_box.rotate_R(pink.resolve().rotation.R)\n    pink_box.move(*pink.resolve().translation)\n\n    camera_object.rotate_R(camera.calibration.extrinsics.resolve().rotation.R)\n    camera_object.move(*camera.calibration.extrinsics.resolve().translation)\n\n\nrosys.on_repeat(update, interval=0.01)\n\nui.button('Toggle frame', on_click=lambda: pink.in_frame(\n    None if pink.frame_id == 'blue' else blue\n))\n\nui.run(title='RoSys')\n</code></pre> <p></p>"},{"location":"examples/coordinate-frames/#persistence","title":"Persistence","text":"<p>Frames are defined and accessed through an <code>id</code>. The <code>id</code> must be unique and is used lazily whenever a transformation is needed. This <code>id</code> makes it possible to fully persist <code>Pose3d</code>, <code>Frame3d</code> and their relationships. This can be useful for situation where exact relationships are calibrated at runtime (e.g. multi-camera calibration) or when a robot's axes are expected to be persistent across reboots.</p> <p>For the second case, make sure that you know when the persistence overwrite happens (in the rosys startup handler) so that everything is loaded correctly. See examples/camera_arm for an example on that.</p>"},{"location":"examples/hardware/","title":"Hardware","text":"<p>The other examples use simulated hardware for simplicity and easy execution on any development system. To be able to control real hardware we recommend to derive a <code>Simulation</code> and <code>Hardware</code> version from a shared interface. Depending on your environment you can then instantiate the correct implementation without bothering with it in the rest of your code.</p>"},{"location":"examples/hardware/#custom-implementation","title":"Custom Implementation","text":"<p>For a differential-steering controlled robot, RoSys offers a <code>Wheels</code> base class plus a <code>WheelsSimulation</code>. The following example illustrates how to implement a <code>CustomWheelsHardware</code> module that derives from <code>Wheels</code>, reads the currrent velocity regularly and can be steered with linear and angular velocity.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nimport rosys\n\n\nclass CustomWheelsHardware(rosys.hardware.Wheels):\n\n    def __init__(self) -&gt; None:\n        super().__init__()\n        rosys.on_repeat(self.read_current_velocity, 0.01)\n\n    async def drive(self, linear: float, angular: float) -&gt; None:\n        await super().drive(linear, angular)\n        # TODO send hardware command to drive with given linear and angular velocity\n\n    async def stop(self) -&gt; None:\n        await super().stop()\n        # TODO send hardware command to stop the wheels\n\n    async def read_current_velocity(self) -&gt; None:\n        velocities: list[rosys.geometry.Velocity] = []\n        # TODO: read measured velocities from the hardware\n        self.VELOCITY_MEASURED.emit(velocities)\n\n\ntry:\n    wheels = CustomWheelsHardware()\n    robot = rosys.hardware.Robot([wheels])\nexcept Exception:\n    wheels = rosys.hardware.WheelsSimulation()\n    robot = rosys.hardware.RobotSimulation([wheels])\nodometer = rosys.driving.Odometer(wheels)\nsteerer = rosys.driving.Steerer(wheels)\n\nrosys.driving.keyboard_control(steerer)\nrosys.driving.joystick(steerer)\nui.label().bind_text_from(wheels, 'linear_target_speed',\n                          lambda l: f'Linear: {l:.2f} m/s')\nui.label().bind_text_from(wheels, 'angular_target_speed',\n                          lambda a: f'Angular: {a:.2f} rad/s')\n\nui.run(title='RoSys')\n</code></pre> <p>Depending on your hardware you may need to modify a PWM signal, send commands via CAN bus or serial, use Protobuf over Ethernet or something else. By raising an exception if the real hardware is not available, a simulated robot is instantiated instead. The robot can be controlled by keyboard or joystick.</p>"},{"location":"examples/hardware/#robot-brain","title":"Robot Brain","text":"<p>The Zauberzeug Robot Brain is an industrial-grade controller which combines artificial intelligence with machinery. It has a built-in ESP32 microcontroller with Lizard installed to do the actual hardware communication in realtime.</p> <p>Serial communication is used to send and receive messages between the built-in NVidia Jetson and the microcontroller. You can call <code>SerialCommunication.is_possible()</code> to automatically switch between simulation and real hardware. The module <code>WheelsHardware</code> expects a <code>RobotBrain</code>, which controls the <code>SerialCommunication</code> with the microcontroller.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nfrom rosys.driving import Odometer, Steerer, joystick, keyboard_control\nfrom rosys.hardware import (\n    CanHardware,\n    RobotBrain,\n    RobotHardware,\n    RobotSimulation,\n    SerialCommunication,\n    WheelsHardware,\n    WheelsSimulation,\n    communication,\n)\n\nis_real = SerialCommunication.is_possible()\nif is_real:\n    communication = SerialCommunication()\n    robot_brain = RobotBrain(communication)\n    can = CanHardware(robot_brain)\n    wheels = WheelsHardware(robot_brain, can=can)\n    robot = RobotHardware([can, wheels], robot_brain)\nelse:\n    wheels = WheelsSimulation()\n    robot = RobotSimulation([wheels])\nodometer = Odometer(wheels)\nsteerer = Steerer(wheels)\n\nkeyboard_control(steerer)\njoystick(steerer)\n\nif is_real:\n    communication.debug_ui()\n    robot_brain.developer_ui()\n\nui.run(title='RoSys')\n</code></pre> <p>With <code>communication.debug_ui()</code> you can add some helpful UI elements for debugging the serial communication. Furthermore, with <code>robot_brain.developer_ui()</code> you can add UI elements to configure and reboot Lizard.</p> <p>The Lizard configuration for a differential-steering controlled robot with an ODrive might look as follows:</p> <pre><code>can = Can(32, 33, 1000000)\n\nl = ODriveMotor(can, 0x000)\nr = ODriveMotor(can, 0x100)\nl.m_per_tick = 0.0627\nr.m_per_tick = 0.0627\n\nwheels = ODriveWheels(l, r)\nwheels.width = 0.515\n\ncore.output(\"core.millis wheels.linear_speed:3 wheels.angular_speed:3\")\n</code></pre>"},{"location":"examples/hello_bot/","title":"Hello Bot","text":"<p>The hello_bot is a minimalistic example bot.</p> <p>To get started, navigate to the hello_bot directory and execute</p> <pre><code>docker-compose up\n</code></pre> <p>In the terminal you should see a verification message that the server is running. Open a browser at location http://localhost:8080.</p> <pre><code>#!/usr/bin/env python3\nimport os\n\nimport log_configuration\nfrom nicegui import ui\n\nimport rosys\nfrom rosys.automation import Automator, automation_controls\nfrom rosys.driving import Driver, Odometer, PathSegment, Steerer, joystick, keyboard_control, robot_object\nfrom rosys.geometry import Point, Prism, Spline\nfrom rosys.hardware import (\n    CanHardware,\n    RobotBrain,\n    RobotHardware,\n    RobotSimulation,\n    SerialCommunication,\n    WheelsHardware,\n    WheelsSimulation,\n)\n\n\nasync def drive_square() -&gt; None:\n    await driver.drive_path([\n        PathSegment(spline=Spline.from_points(Point(x=0, y=0), Point(x=4, y=0))),\n        PathSegment(spline=Spline.from_points(Point(x=4, y=0), Point(x=4, y=4))),\n        PathSegment(spline=Spline.from_points(Point(x=4, y=4), Point(x=0, y=4))),\n        PathSegment(spline=Spline.from_points(Point(x=0, y=4), Point(x=0, y=0))),\n    ], stop_at_end=True)\n\nlog_configuration.setup()\n\n# setup\nshape = Prism.default_robot_shape()\nif SerialCommunication.is_possible():\n    communication = SerialCommunication()\n    robot_brain = RobotBrain(communication)\n    can = CanHardware(robot_brain)\n    wheels = WheelsHardware(robot_brain, can=can)\n    robot = RobotHardware([can, wheels], robot_brain)\nelse:\n    wheels = WheelsSimulation()\n    robot = RobotSimulation([wheels])\nsteerer = Steerer(wheels)\nodometer = Odometer(wheels)\ndriver = Driver(wheels, odometer)\nautomator = Automator(steerer, default_automation=drive_square,\n                      on_interrupt=wheels.stop)\n\n# ui\nwith ui.card():\n    keyboard_control(steerer)\n\n    with ui.row():\n        state = ui.label()\n        ui.timer(0.1, lambda: state.set_text(\n            f'{rosys.time():.3f} s, {odometer.prediction}'\n        ))\n\n    with ui.row():\n        with ui.scene():\n            robot_object(shape, odometer)\n        joystick(steerer, size=50, color='blue')\n\n    with ui.row():\n        automation_controls(automator)\n        if isinstance(wheels, WheelsHardware):\n            ui.button('configure microcontroller',\n                      on_click=robot_brain.configure).props('outline')\n        ui.button('restart rosys', on_click=lambda: os.utime('main.py')) \\\n            .props('outline')\n\n# start\nui.run(title='hello_bot')\n</code></pre>"},{"location":"examples/kpi/","title":"KPI","text":"<p>This example demonstrates how to create a KPI page.</p> <pre><code>#!/usr/bin/env python3\nfrom datetime import datetime, timedelta\nfrom random import randint\n\nfrom nicegui import ui\n\nfrom rosys.analysis import Day, KpiChart, KpiLogger, date_to_str\nfrom rosys.analysis import kpi_page as rosys_kpi_page\n\n\ndef generate_example_kpis(logger: KpiLogger) -&gt; None:\n    logger.days = [\n        Day(\n            date=date_to_str(datetime.today().date() - timedelta(days=i)),\n            incidents={\n                'wheels_slipped': randint(0, 100),\n                'wheels_blocking': randint(0, 15),\n                'tests_run': randint(0, 30),\n                'finished_task': randint(0, 5),\n                'connection_established': randint(0, 200),\n                'bumps': randint(0, 50),\n            },\n        ) for i in range(7 * 3)\n    ][::-1]\n\n\nclass kpi_page(rosys_kpi_page):\n\n    @property\n    def title(self) -&gt; str:\n        return 'KPI Example Page'\n\n    @property\n    def charts(self) -&gt; list[KpiChart]:\n        positives = KpiChart(title='Positive Events', indicators={\n            'finished_task': 'Finished Tasks',\n        }, colormap='Greens')\n        neutral_events = KpiChart(title='Neutral Events', indicators={\n            'connection_established': 'Connection was established',\n            'tests_run': 'Tests',\n        })\n        negatives = KpiChart(title='Exceptions', indicators={\n            'wheels_slipped': 'Wheels Slipping',\n            'wheels_blocking': 'Wheels Blocking',\n            'bumps': 'Robot bumped into something',\n        }, colormap='Reds')\n        return [positives, neutral_events, negatives]\n\n    @property\n    def timespans(self) -&gt; dict[int, str]:\n        return {\n            7: '7 days',\n            14: '2 weeks',\n            21: '3 weeks',\n        }\n\n\nkpi_logger = KpiLogger()\ngenerate_example_kpis(kpi_logger)\nkpi_page(kpi_logger)\n\nui.run(title='KPI Example')\n</code></pre>"},{"location":"examples/navigation/","title":"Navigation","text":"<p>This example is similar to Click-and-drive but includes a <code>PathPlanner</code> to find a path around an obstacle.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\nfrom nicegui.events import SceneClickEventArguments\n\nfrom rosys.automation import Automator\nfrom rosys.driving import Driver, Odometer, robot_object\nfrom rosys.geometry import Point, Pose, Prism\nfrom rosys.hardware import RobotSimulation, WheelsSimulation\nfrom rosys.pathplanning import Obstacle, PathPlanner, obstacle_object, path_object\n\nshape = Prism.default_robot_shape()\nPathPlanner.USE_PERSISTENCE = False\npath_planner = PathPlanner(shape)\npath_planner.obstacles['0'] = Obstacle(id='0', outline=[Point(x=3, y=0),\n                                                        Point(x=0, y=3),\n                                                        Point(x=3, y=3)])\nwheels = WheelsSimulation()\nrobot = RobotSimulation([wheels])\nodometer = Odometer(wheels)\ndriver = Driver(wheels, odometer)\nautomator = Automator(None, on_interrupt=wheels.stop)\n\n\nasync def handle_click(e: SceneClickEventArguments):\n    for hit in e.hits:\n        if hit.object_id == 'ground':\n            goal = Pose(x=hit.x, y=hit.y,\n                        yaw=odometer.prediction.direction(Point(x=hit.x, y=hit.y)))\n            path = await path_planner.search(start=odometer.prediction, goal=goal)\n            path3d.update(path)\n            automator.start(driver.drive_path(path))\n\nwith ui.scene(on_click=handle_click, width=600):\n    robot_object(shape, odometer)\n    obstacle_object(path_planner)\n    path3d = path_object()\n\nui.label('click into the scene to drive the robot')\n\nui.run(title='RoSys')\n</code></pre>"},{"location":"examples/navigation/#path-following","title":"Path Following","text":"<p>When following a path, a \"carrot\" is dragged along a spline and the robot follows it like a donkey. Additionally, there is a virtual \"hook\" attached to the robot, which is pulled towards the carrot.</p> <p>There are three parameters:</p> <ul> <li><code>hook_offset</code>: How far from the wheel axis (i.e. the coordinate center of the robot) is the hook, which is pulled towards the carrot.</li> <li><code>carrot_offset</code>: How far ahead of the carrot is the robot pulled. This parameter is necessary in order to have the hook pulled a bit further, even though the carrot already reached the end of the spline.</li> <li><code>carrot_distance</code>: How long is the \"thread\" between hook and carrot (or the offset point ahead of the carrot, respectively).</li> </ul> <p>In the following illustration these points are depicted as spheres: the coordinate center of the robot (blue, small), the hook (blue, large), carrot (orange, small), offset point ahead of the carrot (orange, large).</p> <p></p> <p>You can display a wire frame version of the robot by passing <code>debug=true</code> to the <code>robot_object</code>.</p> <p>Note</p> <p>The automation <code>drive_spline</code> has an optional argument <code>flip_hook</code>. It turns the hook 180 degrees to the back of the robot, while preserving the distance <code>hook_offset</code> to the robot's coordinate center. This allows the robot to drive backwards to a point behind it instead of turning around and approaching it forwards.</p> <p>A more complex example can be found in the RoSys GitHub repository. There you can create new obstacles and choose between straight driving or navigation.</p>"},{"location":"examples/obstacles/","title":"Obstacle Demo","text":"<p>This example shows how to use path planning and obstacle avoidance.</p> <p>To get started, navigate to the obstacles directory and execute</p> <pre><code>docker-compose up\n</code></pre> <p>In the terminal you should see a verification message that the server is running. Open a browser at location http://localhost:8080.</p> <pre><code>#!/usr/bin/env python3\nimport os\nimport uuid\n\nfrom nicegui import ui\nfrom nicegui.events import SceneClickEventArguments\n\nimport rosys\nfrom rosys.automation import Automator, automation_controls\nfrom rosys.driving import Driver, Odometer, PathSegment, Steerer, keyboard_control, robot_object\nfrom rosys.geometry import Point, Pose, Prism, Spline\nfrom rosys.hardware import (\n    CanHardware,\n    RobotBrain,\n    RobotHardware,\n    RobotSimulation,\n    SerialCommunication,\n    WheelsHardware,\n    WheelsSimulation,\n)\nfrom rosys.pathplanning import Obstacle, PathPlanner, obstacle_object, path_object\n\n# setup\nshape = Prism(outline=[(0, 0), (-0.5, -0.5), (1.5, -0.5), (1.75, 0), (1.5, 0.5), (-0.5, 0.5)], height=0.5)\nif SerialCommunication.is_possible():\n    communication = SerialCommunication()\n    robot_brain = RobotBrain(communication)\n    can = CanHardware(robot_brain)\n    wheels = WheelsHardware(robot_brain, can=can)\n    robot = RobotHardware([can, wheels], robot_brain)\nelse:\n    wheels = WheelsSimulation()\n    robot = RobotSimulation([wheels])\nsteerer = Steerer(wheels)\nodometer = Odometer(wheels)\ndriver = Driver(wheels, odometer)\nautomator = Automator(steerer, on_interrupt=wheels.stop)\npath_planner = PathPlanner(shape)\n\n# ui\nwith ui.card():\n    keyboard_control(steerer)\n\n    state = ui.label()\n    ui.timer(0.1, lambda: state.set_text(\n        f'{rosys.time():.3f} s, {odometer.prediction}'\n    ))\n\n    click_mode = ui.toggle({\n        'drive': 'Drive',\n        'navigate': 'Navigate',\n        'obstacles': 'Obstacles',\n    }, value='drive').props('outline')\n\n    async def handle_click(e: SceneClickEventArguments):\n        if e.click_type != 'dblclick':\n            return\n        for hit in e.hits:\n            object_type = (hit.object_name or '').split('_')[0] or hit.object_id\n            target = Point(x=hit.x, y=hit.y)\n            if object_type == 'ground' and click_mode.value == 'drive':\n                start = odometer.prediction.point\n                path = [PathSegment(spline=Spline(\n                    start=start,\n                    control1=start.interpolate(target, 1/3),\n                    control2=start.interpolate(target, 2/3),\n                    end=target,\n                ))]\n                path3d.update(path)\n                automator.start(driver.drive_path(path))\n                return\n            if object_type == 'ground' and click_mode.value == 'navigate':\n                goal = Pose(x=hit.x, y=hit.y,\n                            yaw=odometer.prediction.direction(target))\n                path = await path_planner.search(start=odometer.prediction,\n                                                 goal=goal, timeout=3.0)\n                path3d.update(path)\n                automator.start(driver.drive_path(path))\n                return\n            if object_type == 'ground' and click_mode.value == 'obstacles':\n                id_ = str(uuid.uuid4())\n                path_planner.obstacles[id_] = Obstacle(id=id_, outline=[\n                    Point(x=hit.x-0.5, y=hit.y-0.5),\n                    Point(x=hit.x+0.5, y=hit.y-0.5),\n                    Point(x=hit.x-0.5, y=hit.y+0.5),\n                ])\n                path_planner.request_backup()\n                path_planner.OBSTACLES_CHANGED.emit(path_planner.obstacles)\n                return\n            if object_type == 'obstacle' and click_mode.value == 'obstacles':\n                del path_planner.obstacles[hit.object.name.split('_')[1]]\n                path_planner.request_backup()\n                path_planner.OBSTACLES_CHANGED.emit(path_planner.obstacles)\n                return\n\n    with ui.scene(640, 480, on_click=handle_click) as scene:\n        robot_object(shape, odometer, debug=True)\n        obstacle_object(path_planner)\n        path3d = path_object()\n\n    with ui.row():\n        automation_controls(automator)\n        ui.button('restart rosys',\n                  on_click=lambda: os.utime('main.py')).props('outline')\n\n# start\nui.run(title='obstacles')\n</code></pre>"},{"location":"examples/path_planning_analysis/","title":"Path Planning Analysis","text":"<p>This example demonstrates how to analyze the path planning results.</p> <pre><code>#!/usr/bin/env python3\nimport logging\nimport time\n\nimport numpy as np\nimport pylab as pl\nfrom nicegui import app, ui\n\n# pylint: disable=unused-import\nfrom rosys.geometry import Point, Pose  # noqa: F401\nfrom rosys.pathplanning import plot_tools as pt\nfrom rosys.pathplanning.area import Area  # noqa: F401\nfrom rosys.pathplanning.delaunay_planner import DelaunayPlanner\nfrom rosys.pathplanning.obstacle import Obstacle  # noqa: F401\nfrom rosys.pathplanning.planner_process import PlannerSearchCommand\nfrom rosys.pathplanning.robot_renderer import RobotRenderer\n\nrobot_shape = ui.input('Robot shape', placeholder='[(x0, y0), (x1, y1), ...]') \\\n    .bind_value(app.storage.general, 'robot_shape') \\\n    .classes('w-full')\nsearch_command = ui.textarea('Search command:',\n                             placeholder='PlannerSearchCommand(...)') \\\n    .bind_value(app.storage.general, 'planner_search_command') \\\n    .classes('w-full')\n\n\ndef run() -&gt; None:\n    if not robot_shape.value or not search_command.value:\n        return\n\n    shape: list[tuple[float, float]] = eval(robot_shape.value)  # pylint: disable=eval-used\n    cmd: PlannerSearchCommand = eval(search_command.value)  # pylint: disable=eval-used\n\n    planner = DelaunayPlanner(shape)\n\n    t = time.time()\n    planner.update_map(cmd.areas, cmd.obstacles,\n                       [cmd.start.point, cmd.goal.point],\n                       deadline=time.time()+10.0)\n    dt0 = time.time() - t\n\n    t = time.time()\n    try:\n        path = planner.search(cmd.start, cmd.goal)\n    except RuntimeError:\n        logging.exception('could not find path')\n        path = []\n    dt1 = time.time() - t\n\n    with plot:\n        pl.clf()\n        pl.title(f'map: {dt0:.3f} s, path: {dt1:.3f} s')\n        pt.show_obstacle_map(planner.obstacle_map)\n        pl.gca().invert_yaxis()\n        pl.autoscale(False)\n        assert planner.tri_points is not None\n        assert planner.tri_mesh is not None\n        pl.triplot(planner.tri_points[:, 0], planner.tri_points[:, 1],\n                   planner.tri_mesh.simplices, lw=0.1)\n\n        pt.plot_path(path, 'C1')\n        robot_renderer = RobotRenderer(shape)\n        pt.plot_robot(robot_renderer, (cmd.start.x, cmd.start.y, cmd.start.yaw),\n                      'C0', lw=2)\n        pt.plot_robot(robot_renderer, (cmd.goal.x, cmd.goal.y, cmd.goal.yaw),\n                      'C0', lw=2)\n        for step in path:\n            for t in [0, 1]:\n                yaw = step.spline.yaw(t) + np.pi if step.backward else step.spline.yaw(t)\n                pt.plot_robot(robot_renderer,\n                              (step.spline.x(t), step.spline.y(t), yaw),\n                              'C2', lw=1)\n\n\nwith ui.row():\n    plot = ui.pyplot(figsize=(8, 8))\n    ui.button('Re-run', on_click=run).props('icon=replay outline')\nrun()\n\nui.run(title='Path Planning Analysis')\n</code></pre>"},{"location":"examples/persistence/","title":"Persistence","text":"<p>The <code>Persistable</code> class is a mixin for objects that need to be persisted. It provides a <code>persistent</code> method that can be used to make the object persistent.</p> <p>Derived classes must implement the <code>backup_to_dict</code> and <code>restore_from_dict</code> methods.</p> <p>Note that the persistence module contains a number of helper functions:</p> <ul> <li><code>to_dict</code>: converts (dictionaries or lists of) dataclasses into a dictionary (or list)</li> <li><code>from_dict</code>: converts a dictionary into a dataclass of given type</li> <li><code>replace_dict</code>: replaces the content of a dictionary using <code>from_dict</code> for each item</li> <li><code>replace_list</code>: replaces the content of a list using <code>from_dict</code> for each item</li> <li><code>replace_set</code>: replaces the content of a set using <code>from_dict</code> for each item</li> <li><code>replace_dataclass</code>: replaces the attributes of a dataclass with the values of a dictionary</li> </ul> <p>Further helper functions can be used for importing and exporting all persistable objects:</p> <ul> <li>The <code>export_all</code> method can be used to export all persistable objects to a dictionary.</li> <li>The <code>import_all</code> method can be used to import all persistable objects from a dictionary.</li> <li>Likewise, <code>export_button</code> and <code>import_button</code> are UI elements based on these two methods that can be added to a page.</li> </ul> <p>By default, data is stored in the <code>~/.rosys</code> directory. The filename is derived from the module name. Both can be changed by setting the <code>path</code> and <code>key</code> parameters of the <code>persistent</code> method.</p> <p>If you want to automatically keep daily backups, you can use the <code>BackupSchedule</code> module. It will backup all the contents of your ~/.rosys directory at a configurable directory and at a given time each day. When a maximum number of backup files is reached (specified with <code>backup_count</code>), it will delete the oldest file.</p> <p>You should choose wisely which values to persist. In particular, avoid to persist volatile things like wheel odometry or other sensor readings to save CPU and IO bandwidth.</p> <pre><code>\n</code></pre>"},{"location":"examples/persistence/#migration-from-the-old-persistentmodule-class","title":"Migration from the old <code>PersistentModule</code> class","text":"<p>The <code>PersistentModule</code> class has been replaced by the <code>Persistable</code> mixin in version 0.24.0. To migrate, follow these steps:</p> <ol> <li>Replace <code>PersistentModule</code> with <code>Persistable</code> in the class definition.</li> <li>Rename the <code>backup</code> and <code>restore</code> methods to <code>backup_to_dict</code> and <code>restore_from_dict</code>.</li> <li>Use the <code>persistent</code> method to make objects persistent which used to be derived from <code>PersistentModule</code>:    <code>KpiLogger</code>, <code>Schedule</code>, <code>PathPlanner</code>, <code>CameraProvider</code> and derived classes.</li> <li>If you called <code>PersistentModule</code> with a <code>persistence_key</code>,    remove it and use the <code>key</code> parameter of the <code>persistent</code> method instead.</li> </ol>"},{"location":"examples/play-pause-stop/","title":"Play-pause-stop","text":"<p>In this example, we use the <code>AutomationControls</code> UI element to start, pause and stop an automation. Here we let the robot drive to predefined checkpoints one after the other.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nimport rosys\nfrom rosys.geometry import Point, Prism\n\n\nasync def run() -&gt; None:\n    for c in checkpoints:\n        await driver.drive_to(c)\n\ncheckpoints: list[Point] = [Point(x=-3, y=1), Point(x=3, y=3), Point(x=2, y=-2)]\nwheels = rosys.hardware.WheelsSimulation()\nrobot = rosys.hardware.RobotSimulation([wheels])\nodometer = rosys.driving.Odometer(wheels)\ndriver = rosys.driving.Driver(wheels, odometer)\nautomator = rosys.automation.Automator(None, default_automation=run,\n                                       on_interrupt=wheels.stop)\n\nwith ui.scene(width=600).classes('drop-shadow-lg') as scene:\n    rosys.driving.robot_object(Prism.default_robot_shape(), odometer)\n    for i, point in enumerate(checkpoints):\n        scene.text(f'{i+1}').move(x=point.x, y=point.y)\n\nwith ui.row():\n    rosys.automation.automation_controls(automator)\n\nui.run(title='RoSys')\n</code></pre> <p>To achieve this, we define our automation as an async method and pass it to the <code>default_automation</code> parameter of the <code>Automator</code>.</p> <p></p>"},{"location":"examples/schedule/","title":"Schedule automations","text":"<p>You can schedule when the robot should be active. The <code>Schedule</code> module comes with its own UI to manipulate the half-hourly time plan.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nfrom rosys.automation import Automator, Schedule, automation_controls\nfrom rosys.driving import Driver, Odometer, robot_object\nfrom rosys.geometry import Point, Prism\nfrom rosys.hardware import RobotSimulation, WheelsSimulation\n\n\nasync def drive_around() -&gt; None:\n    while True:\n        await driver.drive_to(Point(x=1, y=0))\n        await driver.drive_to(Point(x=0, y=0))\n\n\nasync def drive_home() -&gt; None:\n    await driver.drive_to(Point(x=-3, y=0))\n\n\nshape = Prism.default_robot_shape()\nwheels = WheelsSimulation()\nrobot = RobotSimulation([wheels])\nodometer = Odometer(wheels)\ndriver = Driver(wheels, odometer)\nautomator = Automator(None, default_automation=drive_around,\n                      on_interrupt=wheels.stop)\n\nlocations = {\n    (52.520008, 13.404954): 'Berlin',\n    (40.730610, -73.935242): 'New York',\n    None: 'no location',\n}\nschedule = Schedule(automator=automator, on_activate=drive_around,\n                    on_deactivate=drive_home, location=None,\n                    locations=locations, is_enabled=True)\nschedule.fill(False)  # disable at all times so the user can enable it manually\nschedule.is_enabled = True  # the schedule must be enabled to take any effect\n\nwith ui.row().classes('items-end'):\n    schedule.ui()\n    with ui.column().classes('items-end'):\n        with ui.row():\n            automation_controls(automator)\n        with ui.scene(height=360):\n            robot_object(shape, odometer)\n\nui.run(title='RoSys')\n</code></pre> <p>There is also the possibility to pass a geographic location to restrict the activity to daylight only.</p>"},{"location":"examples/simulation-speed/","title":"Simulation Speed","text":"<p>When running in simulation you can accelerate the time. Here we have set up a fence in which the robot moves to random positions. With a simple slider the execution time is accelerated. Note how the time advances faster if the simulation speed is increased. The driving speed of the robot remains the same.</p> <p></p> <p>This is achieved simply by placing <code>rosys.simulation_ui()</code> in your UI. The rest of the code is needed to define the boundary, draw it in the 3D scene and start the automation for random movement:</p> <pre><code>#!/usr/bin/env python3\nimport random\n\nfrom nicegui import ui\n\nimport rosys\nfrom rosys.automation import Automator\nfrom rosys.driving import Driver, Odometer, robot_object\nfrom rosys.geometry import Point, Prism\nfrom rosys.hardware import RobotSimulation, WheelsSimulation\n\nwheels = WheelsSimulation()\nrobot = RobotSimulation([wheels])\nodometer = Odometer(wheels)\ndriver = Driver(wheels, odometer)\ndriver.parameters.linear_speed_limit = 3\ndriver.parameters.angular_speed_limit = 1\nautomator = Automator(None, on_interrupt=wheels.stop)\n\nsize = 3\nboundary = [(-size, -size), (-size, size), (size, size), (size, -size)]\n\nwith ui.scene() as scene:\n    robot_object(Prism.default_robot_shape(), odometer)\n    for i, a in enumerate(boundary):\n        b = boundary[(i+1) % len(boundary)]\n        ui.scene.line([*a, 0.1], [*b, 0.1]).material('red')\n    scene.move_camera(0, 0, 8)\nwith ui.column().style('width: 400px'):\n    rosys.simulation_ui()\n\n\nasync def move_around():\n    while True:\n        await driver.drive_to(Point(x=random.uniform(-size, size),\n                                    y=random.uniform(-size, size)))\n\nrosys.on_startup(lambda: automator.start(move_around()))\n\nui.run(title='RoSys')\n</code></pre>"},{"location":"examples/steering/","title":"Steering","text":"<p>The following example simulates a robot that can be steered using keyboard controls or a joystick via web interface.</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nfrom rosys.driving import Odometer, Steerer, joystick, keyboard_control, robot_object\nfrom rosys.geometry import Prism\nfrom rosys.hardware import RobotSimulation, WheelsSimulation\n\nshape = Prism.default_robot_shape()\nwheels = WheelsSimulation()\nrobot = RobotSimulation([wheels])\nodometer = Odometer(wheels)\nsteerer = Steerer(wheels)\n\nkeyboard_control(steerer)\njoystick(steerer, size=50, color='blue')\nwith ui.scene():\n    robot_object(shape, odometer)\n\nui.run(title='RoSys')\n</code></pre> <p></p> Keyboard Control By adding a <code>KeyboardControl</code> to the user interface you enable steering the robot with the keyboard. Press the arrow keys while holding the SHIFT key to steer the robot. You can also modify the speed of the robot by pressing the a number key. Use the optional parameter <code>default_speed</code> to change the initial value. Joystick When operating from a mobile phone, you can use a <code>Joystick</code> to create a UI element with touch control. You can drive the robot by dragging the mouse inside the top left square."},{"location":"getting_started/","title":"Getting Started","text":"<p>First install RoSys with pip or Docker. Then create a directory to host your code and put it under version control. Name your entry file <code>main.py</code> and add the following content:</p> <pre><code>#!/usr/bin/env python3\nfrom nicegui import ui\n\nimport rosys\n\n# setup\nshape = rosys.geometry.Prism.default_robot_shape()\nrosys.hardware.SerialCommunication.search_paths = ['/dev/ttyUSB0']\nis_real = rosys.hardware.SerialCommunication.is_possible()\nwheels: rosys.hardware.Wheels\nrobot: rosys.hardware.Robot\nif is_real:\n    communication = rosys.hardware.SerialCommunication()\n    robot_brain = rosys.hardware.RobotBrain(communication)\n    can = rosys.hardware.CanHardware(robot_brain)\n    wheels = rosys.hardware.WheelsHardware(robot_brain,\n                                           can=can,\n                                           left_can_address=0x100,\n                                           right_can_address=0x000,\n                                           m_per_tick=0.01571,\n                                           width=0.207,\n                                           is_right_reversed=True)\n    robot = rosys.hardware.RobotHardware([can, wheels], robot_brain)\nelse:\n    wheels = rosys.hardware.WheelsSimulation()\n    robot = rosys.hardware.RobotSimulation([wheels])\nodometer = rosys.driving.Odometer(wheels)\nsteerer = rosys.driving.Steerer(wheels)\n\n# ui\nrosys.driving.keyboard_control(steerer)\nwith ui.scene():\n    rosys.driving.robot_object(shape, odometer)\nui.label('hold SHIFT to steer with the keyboard arrow keys')\nif is_real:\n    ui.button('configure microcontroller',\n              on_click=robot_brain.configure).props('outline')\n\n# start\nui.run(title='RoSys')\n</code></pre> <p>If you launch the program, your browser will open the url http://0.0.0.0:8080/ and present a 3d view:</p> <p></p>"},{"location":"getting_started/#explanation","title":"Explanation","text":""},{"location":"getting_started/#imports","title":"Imports","text":"<p>The user interface is built with NiceGUI. The individual RoSys modules come in packages <code>driving</code>, <code>geometry</code>, <code>hardware</code> and others.</p>"},{"location":"getting_started/#setup","title":"Setup","text":"<p>In this example we create a <code>Steerer</code> which needs an <code>Odometer</code>. Here we work without real hardware, so two wheels are simulated. Please see Hardware for an example which can actually be used on a mobile robot. For visualization purposes we also need the approximate robot shape.</p>"},{"location":"getting_started/#user-interface","title":"User Interface","text":"<p>The user interface consists of keyboard control with access to the steerer as well as a 3D view of the scene. The latter only contains the <code>RobotObject</code> with the given shape. The robot pose is constantly updated from the odometer. See NiceGUI for more details about its API.</p>"},{"location":"getting_started/#start","title":"Start","text":"<p>NiceGUI provides a <code>ui.run</code> command which launches the web server and opens the corresponding web application. If you modify the code, a reload is triggered automatically. This is very convenient, but can be deactivated by passing <code>reload=False</code>.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>analysis</li> <li>automation</li> <li>driving</li> <li>geometry</li> <li>hardware</li> <li>helpers</li> <li>pathplanning</li> <li>system</li> <li>vision<ul> <li>camera</li> <li>rtsp_camera</li> <li>simulated_camera</li> <li>usb_camera</li> </ul> </li> </ul>"},{"location":"reference/rosys/analysis/","title":"analysis","text":""},{"location":"reference/rosys/analysis/#rosys.analysis.logging_page","title":"logging_page","text":""},{"location":"reference/rosys/analysis/#rosys.analysis.logging_page.LoggingPage","title":"LoggingPage","text":"<pre><code>LoggingPage(group_names: list[str] | None = None)\n</code></pre> <p>Logging Page</p> <p>This module creates a page to change the log levels of different loggers. A list of logger names like <code>[\"field_friend\", \"rosys\"]</code> can be passed to group them together. It is mounted at /logging.</p>"},{"location":"reference/rosys/analysis/#rosys.analysis.profile_button","title":"profile_button","text":"<pre><code>profile_button()\n</code></pre> <p>               Bases: <code>button</code></p> <p>The profile button allows starting and stopping a profiling session.</p> <p>Use the <code>profiling.profile</code> decorator for including functions or methods in the analysis. The results are shown on the console.</p>"},{"location":"reference/rosys/automation/","title":"automation","text":""},{"location":"reference/rosys/automation/#rosys.automation.Automator","title":"Automator","text":"<pre><code>Automator(\n    steerer: Steerer | None,\n    *,\n    default_automation: Callable | None = None,\n    on_interrupt: Callable | None = None\n)\n</code></pre> <p>An automator allows running automations, i.e. coroutines that can be paused and resumed.</p> <p>See Click-and-drive for a simple example of an automation.</p> <p>steerer: If provided, manually steering the robot will pause a currently running automation.</p> <p>default_automation: If provided, it allows the automator to start a new automation without passing an automation (e.g. via an \"Play\"-button like offered by the automation controls). The passed function should return a new coroutine on every call (see Play-pause-stop example).</p> <p>on_interrupt: Optional callback that will be called when an automation pauses or stops (the cause is provided as string parameter).</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.AUTOMATION_COMPLETED","title":"AUTOMATION_COMPLETED  <code>instance-attribute</code>","text":"<pre><code>AUTOMATION_COMPLETED = Event[[]]()\n</code></pre> <p>an automation has been completed</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.AUTOMATION_FAILED","title":"AUTOMATION_FAILED  <code>instance-attribute</code>","text":"<pre><code>AUTOMATION_FAILED = Event[str]()\n</code></pre> <p>an automation has failed to complete (string argument: description of the cause)</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.AUTOMATION_PAUSED","title":"AUTOMATION_PAUSED  <code>instance-attribute</code>","text":"<pre><code>AUTOMATION_PAUSED = Event[str]()\n</code></pre> <p>an automation has been paused (string argument: description of the cause)</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.AUTOMATION_RESUMED","title":"AUTOMATION_RESUMED  <code>instance-attribute</code>","text":"<pre><code>AUTOMATION_RESUMED = Event[[]]()\n</code></pre> <p>an automation has been resumed</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.AUTOMATION_STARTED","title":"AUTOMATION_STARTED  <code>instance-attribute</code>","text":"<pre><code>AUTOMATION_STARTED = Event[[]]()\n</code></pre> <p>an automation has been started</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.AUTOMATION_STOPPED","title":"AUTOMATION_STOPPED  <code>instance-attribute</code>","text":"<pre><code>AUTOMATION_STOPPED = Event[str]()\n</code></pre> <p>an automation has been stopped (string argument: description of the cause)</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.disable","title":"disable","text":"<pre><code>disable(because: str) -&gt; None\n</code></pre> <p>Disables the automator.</p> <p>No automations can be started while the automator is disabled. If an automation is running or paused it will be stopped. You need to provide a cause which will be used as notification message.</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.enable","title":"enable","text":"<pre><code>enable() -&gt; None\n</code></pre> <p>Enables the automator.</p> <p>It is enabled by default. It can be disabled by calling <code>disable()</code>.</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.pause","title":"pause","text":"<pre><code>pause(because: str) -&gt; None\n</code></pre> <p>Pauses the current automation.</p> <p>You need to provide a cause which will be used as notification message.</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.resume","title":"resume","text":"<pre><code>resume() -&gt; None\n</code></pre> <p>Resumes the current automation.</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.set_default_automation","title":"set_default_automation","text":"<pre><code>set_default_automation(\n    default_automation: Callable | None,\n) -&gt; None\n</code></pre> <p>Sets the default automation.</p> <p>You can pass a function that returns a new coroutine on every call.</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.start","title":"start","text":"<pre><code>start(\n    coro: Coroutine | None = None, *, paused: bool = False\n) -&gt; None\n</code></pre> <p>Starts a new automation.</p> <p>You can pass any coroutine. The automator will make sure it can be paused, resumed and stopped.</p>"},{"location":"reference/rosys/automation/#rosys.automation.Automator.stop","title":"stop","text":"<pre><code>stop(because: str) -&gt; None\n</code></pre> <p>Stops the current automation.</p> <p>You need to provide a cause which will be used as notification message.</p>"},{"location":"reference/rosys/automation/#rosys.automation.app_controls","title":"app_controls","text":"<pre><code>app_controls(robot_brain: RobotBrain, automator: Automator)\n</code></pre> <p>The AppControls module enables the connection with a mobile-app-based user interface.</p> <p>It uses a given RobotBrain object to communicate with Lizard running on a microcontroller and in turn being connected to a mobile app via Bluetooth Low Energy. It displays buttons to control a given automator.</p>"},{"location":"reference/rosys/automation/#rosys.automation.app_controls.APP_CONNECTED","title":"APP_CONNECTED  <code>instance-attribute</code>","text":"<pre><code>APP_CONNECTED = Event[[]]()\n</code></pre> <p>an app connected via bluetooth (used to refresh information or similar)</p>"},{"location":"reference/rosys/automation/#rosys.automation.app_controls.notify","title":"notify  <code>async</code>","text":"<pre><code>notify(msg: str) -&gt; None\n</code></pre> <p>show notification as Snackbar message on mobile device</p>"},{"location":"reference/rosys/automation/#rosys.automation.app_controls.set_info","title":"set_info  <code>async</code>","text":"<pre><code>set_info(msg: str) -&gt; None\n</code></pre> <p>replace constantly shown info text on mobile device</p>"},{"location":"reference/rosys/automation/#rosys.automation.automation_controls","title":"automation_controls","text":"<pre><code>automation_controls(automator: Automator)\n</code></pre> <p>This UI element contains start/stop/pause/resume buttons for controlling a given automator.</p> <p>See Play-pause-stop for a simple example of the automation controls.</p>"},{"location":"reference/rosys/automation/#rosys.automation.parallelize","title":"parallelize","text":""},{"location":"reference/rosys/automation/#rosys.automation.parallelize.parallelize","title":"parallelize","text":"<pre><code>parallelize(\n    *coros: Coroutine,\n    return_when_first_completed: bool = False\n)\n</code></pre> <p>Parallelize multiple coroutines.</p> <p>This class allows to combine multiple coroutines into one that can be passed to the <code>automator &lt;https://rosys.io/reference/rosys/automation/#rosys.automation.Automator&gt;</code>__ to run them in parallel.</p>"},{"location":"reference/rosys/driving/","title":"driving","text":""},{"location":"reference/rosys/driving/#rosys.driving.Driver","title":"Driver","text":"<pre><code>Driver(wheels: Drivable, odometer: Odometer | PoseProvider)\n</code></pre> <p>The driver module allows following a given path.</p> <p>It requires a wheels module (or any drivable hardware representation) to execute individual drive commands. It also requires an odometer to get a current prediction of the robot's pose. Its <code>parameters</code> allow controlling the specific drive behavior.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Driver.prediction","title":"prediction  <code>property</code>","text":"<pre><code>prediction: Pose\n</code></pre> <p>The current prediction of the robot's pose based on the odometer.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Driver.abort","title":"abort","text":"<pre><code>abort() -&gt; None\n</code></pre> <p>Abort the current drive routine.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Driver.drive_circle","title":"drive_circle  <code>async</code>","text":"<pre><code>drive_circle(\n    target: Point,\n    *,\n    angle_threshold: float = np.deg2rad(5),\n    backward: bool = False,\n    stop_at_end: bool = True\n) -&gt; None\n</code></pre> <p>Drive in a circular path.</p> <p>When the angle between the robot's current direction and the target direction is less than <code>angle_threshold</code>, the robot stops driving.</p> <p>:param target: The target point to drive towards. :param angle_threshold: The angle threshold to stop driving (radians, default: 5\u00b0). :param backward: Whether to drive backwards (default: <code>False</code>). :param stop_at_end: Whether to stop the robot at the end of the circular path (default: <code>False</code>). :raises: DrivingAbortedException: If the driving process is aborted.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Driver.drive_path","title":"drive_path  <code>async</code>","text":"<pre><code>drive_path(\n    path: list[PathSegment],\n    *,\n    throttle_at_end: bool = True,\n    stop_at_end: bool = True\n) -&gt; None\n</code></pre> <p>Drive along a given path.</p> <p>:param path: The path to drive along, composed of PathSegments. :param throttle_at_end: Whether to throttle down when approaching the end of the path (default: <code>True</code>). :param stop_at_end: Whether to stop at the end of the path (default: <code>True</code>). :raises: DrivingAbortedException: If the driving process is aborted.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Driver.drive_spline","title":"drive_spline  <code>async</code>","text":"<pre><code>drive_spline(\n    spline: Spline,\n    *,\n    flip_hook: bool = False,\n    throttle_at_end: bool = True,\n    stop_at_end: bool = True\n) -&gt; None\n</code></pre> <p>Drive along a given spline.</p> <p>:param spline: The spline to drive along. :param flip_hook: Whether to flip the hook offset (default: <code>False</code>). :param throttle_at_end: Whether to throttle down when approaching the end of the spline (default: <code>True</code>). :param stop_at_end: Whether to stop at the end of the spline (default: <code>True</code>). :raises DrivingAbortedException: If the driving process is aborted.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Driver.drive_to","title":"drive_to  <code>async</code>","text":"<pre><code>drive_to(\n    target: Point,\n    *,\n    backward: bool = False,\n    throttle_at_end: bool = True,\n    stop_at_end: bool = True\n) -&gt; None\n</code></pre> <p>Drive to a given target point.</p> <p>:param target: The target point to drive to. :param backward: Whether to drive backwards (default: <code>False</code>). :param throttle_at_end: Whether to throttle down when approaching the target point (default: <code>True</code>). :param stop_at_end: Whether to stop at the target point (default: <code>True</code>). :raises: DrivingAbortedException: If the driving process is aborted.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Odometer","title":"Odometer","text":"<pre><code>Odometer(wheels: VelocityProvider)\n</code></pre> <p>An odometer collects velocity information from a given wheels module (or any velocity-providing hardware representation).</p> <p>It can also handle \"detections\", i.e. absolute pose information with timestamps. Given the history of previously received velocities, it can update its prediction of the current pose.</p> <p>The <code>get_pose</code> method provides robot poses from the within the last 10 seconds.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Odometer.PREDICTION_UPDATED","title":"PREDICTION_UPDATED  <code>instance-attribute</code>","text":"<pre><code>PREDICTION_UPDATED = Event[[]]()\n</code></pre> <p>the pose prediction has been updated</p>"},{"location":"reference/rosys/driving/#rosys.driving.Odometer.WHEELS_TURNED","title":"WHEELS_TURNED  <code>instance-attribute</code>","text":"<pre><code>WHEELS_TURNED = Event[[]]()\n</code></pre> <p>the wheels have turned with non-zero velocity</p>"},{"location":"reference/rosys/driving/#rosys.driving.Steerer","title":"Steerer","text":"<pre><code>Steerer(wheels: Drivable, speed_scaling: float = 1.0)\n</code></pre> <p>The steerer module translates x-y information (e.g. from a joystick) to linear/angular velocities sent to the robot.</p> <p>The wheels module can be any drivable hardware representation. Changing the steering state emits events that can be used to react to manual user interaction.</p>"},{"location":"reference/rosys/driving/#rosys.driving.Steerer.STEERING_STARTED","title":"STEERING_STARTED  <code>instance-attribute</code>","text":"<pre><code>STEERING_STARTED = Event[[]]()\n</code></pre> <p>steering has started</p>"},{"location":"reference/rosys/driving/#rosys.driving.Steerer.STEERING_STOPPED","title":"STEERING_STOPPED  <code>instance-attribute</code>","text":"<pre><code>STEERING_STOPPED = Event[[]]()\n</code></pre> <p>steering has stopped</p>"},{"location":"reference/rosys/driving/#rosys.driving.driver_object","title":"driver_object","text":""},{"location":"reference/rosys/driving/#rosys.driving.driver_object.DriverObject","title":"DriverObject","text":"<pre><code>DriverObject(driver: Driver)\n</code></pre> <p>               Bases: <code>Group</code></p> <p>The DriverObject UI element displays the path following process in a 3D scene.</p> <p>The current pose is taken from a given odometer. An optional driver module shows debugging information about a current path-following process. The <code>debug</code> argument can be set to show a wireframe instead of a closed polygon.</p>"},{"location":"reference/rosys/driving/#rosys.driving.joystick","title":"joystick","text":"<pre><code>joystick(steerer: Steerer, **options)\n</code></pre> <p>               Bases: <code>joystick</code></p> <p>The Joystick UI element allows controlling a given steerer via touch events.</p>"},{"location":"reference/rosys/driving/#rosys.driving.keyboard_control","title":"keyboard_control","text":"<pre><code>keyboard_control(\n    steerer: Steerer,\n    *,\n    default_speed: float = 2.0,\n    connection_timeout: float = 1.0,\n    check_connection_interval: float = 1.0\n)\n</code></pre> <p>The KeyboardControl UI element allows controlling a given steerer via keyboard events.</p> <p>Hold shift while pressing an arrow key to steer the robot. You can change the speed with the number keys 1 to 9 and the initial speed via the <code>default_speed</code> argument.</p>"},{"location":"reference/rosys/driving/#rosys.driving.keyboard_control.CONNECTION_INTERRUPTED","title":"CONNECTION_INTERRUPTED  <code>instance-attribute</code>","text":"<pre><code>CONNECTION_INTERRUPTED = Event[[]]()\n</code></pre> <p>the keyboard control has lost connection to the browser.</p>"},{"location":"reference/rosys/driving/#rosys.driving.robot_object","title":"robot_object","text":"<pre><code>robot_object(\n    shape: Prism, odometer: Odometer, *, debug: bool = False\n)\n</code></pre> <p>               Bases: <code>Group</code></p> <p>The RobotObject UI element displays the robot with its given shape in a 3D scene.</p> <p>The current pose is taken from a given odometer. The <code>debug</code> argument can be set to show a wireframe instead of a closed polygon.</p>"},{"location":"reference/rosys/driving/#rosys.driving.robot_object.with_stl","title":"with_stl","text":"<pre><code>with_stl(\n    url: str,\n    *,\n    x: float = 0,\n    y: float = 0,\n    z: float = 0,\n    omega: float = 0,\n    phi: float = 0,\n    kappa: float = 0,\n    scale: float = 1.0,\n    color: str = \"#ffffff\",\n    opacity: float = 1.0\n) -&gt; RobotObject\n</code></pre> <p>Sets an STL to be displayed as the robot.</p> <p>The file can be served from a local directory with app.add_static_files(url, path).</p>"},{"location":"reference/rosys/geometry/","title":"geometry","text":""},{"location":"reference/rosys/geometry/#rosys.geometry.axes_object","title":"axes_object","text":"<pre><code>axes_object(\n    frame: Pose3d,\n    *,\n    name: str = \"\",\n    show_x: bool = True,\n    show_y: bool = True,\n    show_z: bool = True,\n    length: float = 1.0\n)\n</code></pre> <p>               Bases: <code>group</code></p> <p>An object for visualizing the coordinate frame of a 3D pose in a NiceGUI scene.</p>"},{"location":"reference/rosys/hardware/","title":"hardware","text":""},{"location":"reference/rosys/hardware/#rosys.hardware.Bms","title":"Bms","text":"<pre><code>Bms(**kwargs)\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>The BMS module communicates with a simple battery management system over a serial connection.</p> <p>The BMS module provides measured voltages as an event.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Bms.is_above_percent","title":"is_above_percent","text":"<pre><code>is_above_percent(value: float) -&gt; bool\n</code></pre> <p>Returns whether the battery is charged above the given percentage.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Bms.is_above_voltage","title":"is_above_voltage","text":"<pre><code>is_above_voltage(value: float) -&gt; bool\n</code></pre> <p>Returns whether the battery voltage is above the given value.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Bms.is_below_percent","title":"is_below_percent","text":"<pre><code>is_below_percent(value: float) -&gt; bool\n</code></pre> <p>Returns whether the battery is charged below the given percentage.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Bms.is_below_voltage","title":"is_below_voltage","text":"<pre><code>is_below_voltage(value: float) -&gt; bool\n</code></pre> <p>Returns whether the battery voltage is below the given value.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.BmsHardware","title":"BmsHardware","text":"<pre><code>BmsHardware(\n    robot_brain: RobotBrain,\n    *,\n    expander: ExpanderHardware | None = None,\n    name: str = \"bms\",\n    rx_pin: int = 26,\n    tx_pin: int = 27,\n    baud: int = 9600,\n    num: int = 1,\n    charge_detect_threshold: float = -0.4\n)\n</code></pre> <p>               Bases: <code>Bms</code>, <code>ModuleHardware</code></p> <p>This module implements the hardware interface for the BMS module.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.BmsSimulation","title":"BmsSimulation","text":"<pre><code>BmsSimulation(\n    is_charging: Callable[[], bool] | None = None,\n    fixed_voltage: float | None = None,\n)\n</code></pre> <p>               Bases: <code>Bms</code>, <code>ModuleSimulation</code></p> <p>This module simulates a BMS module.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Bumper","title":"Bumper","text":"<pre><code>Bumper(estop: EStop | None, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>A module that detects when a bumper is triggered.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Bumper.BUMPER_TRIGGERED","title":"BUMPER_TRIGGERED  <code>instance-attribute</code>","text":"<pre><code>BUMPER_TRIGGERED = Event[str]()\n</code></pre> <p>a bumper was triggered (argument: the bumper name)</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.BumperHardware","title":"BumperHardware","text":"<pre><code>BumperHardware(\n    robot_brain: RobotBrain,\n    *,\n    expander: ExpanderHardware | None = None,\n    name: str = \"bumper\",\n    pins: dict[str, int],\n    estop: EStop | None = None,\n    inverted: bool = False\n)\n</code></pre> <p>               Bases: <code>Bumper</code>, <code>ModuleHardware</code></p> <p>Hardware implementation of the bumper module.</p> <p>The module expects a dictionary of pin names and pin numbers. If an e-stop is provided, the module will not trigger bumpers if the e-stop is active.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.BumperSimulation","title":"BumperSimulation","text":"<pre><code>BumperSimulation(estop: EStop | None, **kwargs)\n</code></pre> <p>               Bases: <code>Bumper</code>, <code>ModuleSimulation</code></p> <p>Simulation of the bumper module.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Communication","title":"Communication","text":"<pre><code>Communication()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>This abstract module defines an interface for communicating with a microcontroller.</p> <p>Besides sending and receiving messages a communication module provides a property whether communication is possible. It can also provide a piece of debug UI.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.EStop","title":"EStop","text":"<pre><code>EStop(**kwargs)\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>A module that detects when the e-stop is triggered.</p> <p>The module has a boolean field <code>active</code> that is true when the e-stop is triggered.</p> <p>There is also a boolean field <code>is_soft_estop_active</code> that is true when the soft e-stop is active. It can be set to true or false by calling <code>set_soft_estop(active: bool)</code>.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.EStop.ESTOP_RELEASED","title":"ESTOP_RELEASED  <code>instance-attribute</code>","text":"<pre><code>ESTOP_RELEASED = Event[[]]()\n</code></pre> <p>the e-stop was released</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.EStop.ESTOP_TRIGGERED","title":"ESTOP_TRIGGERED  <code>instance-attribute</code>","text":"<pre><code>ESTOP_TRIGGERED = Event[[]]()\n</code></pre> <p>the e-stop was triggered</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.EStopHardware","title":"EStopHardware","text":"<pre><code>EStopHardware(\n    robot_brain: RobotBrain,\n    *,\n    name: str = \"estop\",\n    pins: dict[str, int],\n    inverted: bool = True\n)\n</code></pre> <p>               Bases: <code>EStop</code>, <code>ModuleHardware</code></p> <p>Hardware implementation of the e-stop module.</p> <p>The module expects a dictionary of pin names and pin numbers.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.EStopSimulation","title":"EStopSimulation","text":"<pre><code>EStopSimulation(**kwargs)\n</code></pre> <p>               Bases: <code>EStop</code>, <code>ModuleSimulation</code></p> <p>Simulation of the e-stop module.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.ExpanderHardware","title":"ExpanderHardware","text":"<pre><code>ExpanderHardware(\n    robot_brain: RobotBrain,\n    *,\n    name: str = \"p0\",\n    serial: SerialHardware,\n    boot: int = 25,\n    enable: int = 14\n)\n</code></pre> <p>               Bases: <code>ModuleHardware</code></p> <p>The expander module represents a second ESP microcontroller connected to the core ESP via serial.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Gnss","title":"Gnss","text":"<pre><code>Gnss()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A GNSS module that provides measurements from a GNSS receiver.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Gnss.NEW_MEASUREMENT","title":"NEW_MEASUREMENT  <code>instance-attribute</code>","text":"<pre><code>NEW_MEASUREMENT = Event[GnssMeasurement]()\n</code></pre> <p>a new measurement has been received (argument: <code>GnssMeasurement</code>)</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.GnssHardware","title":"GnssHardware","text":"<pre><code>GnssHardware(\n    *,\n    antenna_pose: Pose | None,\n    reconnect_interval: float = 3.0\n)\n</code></pre> <p>               Bases: <code>Gnss</code></p> <p>This hardware module connects to a Septentrio SimpleRTK3b (Mosaic-H) GNSS receiver.</p> <p>:param antenna_pose: the pose of the main antenna in the robot's coordinate frame (yaw: direction to the auxiliary antenna) :param reconnect_interval: the interval to wait before reconnecting to the device</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.GnssSimulation","title":"GnssSimulation","text":"<pre><code>GnssSimulation(\n    *,\n    wheels: PoseProvider,\n    lat_std_dev: float = 0.01,\n    lon_std_dev: float = 0.01,\n    heading_std_dev: float = 0.01,\n    gps_quality: GpsQuality = GpsQuality.RTK_FIXED\n)\n</code></pre> <p>               Bases: <code>Gnss</code></p> <p>Simulation of a GNSS receiver.</p> <p>:param wheels: the wheels to use for the simulation :param lat_std_dev: the standard deviation of the latitude in meters :param lon_std_dev: the standard deviation of the longitude in meters :param heading_std_dev: the standard deviation of the heading in degrees :param gps_quality: the quality of the GPS signal</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Imu","title":"Imu","text":"<pre><code>Imu(offset_rotation: Rotation | None = None, **kwargs)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>A module that provides measurements from an IMU.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Imu.NEW_MEASUREMENT","title":"NEW_MEASUREMENT  <code>instance-attribute</code>","text":"<pre><code>NEW_MEASUREMENT = Event[ImuMeasurement]()\n</code></pre> <p>a new measurement has been received (argument: ImuMeasurement)</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.ImuHardware","title":"ImuHardware","text":"<pre><code>ImuHardware(\n    robot_brain: RobotBrain,\n    name: str = \"imu\",\n    *,\n    min_gyro_calibration: float = 1.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Imu</code>, <code>ModuleHardware</code></p> <p>A hardware module that handles the communication with an IMU.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.ImuSimulation","title":"ImuSimulation","text":"<pre><code>ImuSimulation(\n    *,\n    wheels: PoseProvider,\n    interval: float = 0.1,\n    roll_noise: float = 0.0,\n    pitch_noise: float = 0.0,\n    yaw_noise: float = 0.0,\n    **kwargs\n)\n</code></pre> <p>               Bases: <code>Imu</code>, <code>ModuleSimulation</code></p> <p>Simulation of an IMU.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Robot","title":"Robot","text":"<pre><code>Robot(modules: list[Module])\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A robot that consists of a number of modules.</p> <p>It can be either a hardware robot or a simulation.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotBrain","title":"RobotBrain","text":"<pre><code>RobotBrain(\n    communication: Communication,\n    *,\n    enable_esp_on_startup: bool = True\n)\n</code></pre> <p>This module manages the communication with a Zauberzeug Robot Brain.</p> <p>It expects a communication object, which is used for the actual read and write operations. Besides providing some basic methods like configuring or restarting the microcontroller, it augments and verifies checksums for each message.</p> <p>It also keeps track of the clock offset between the microcontroller and the host system, which is used to synchronize the hardware time with the system time. The clock offset is calculated by comparing the hardware time with the system time and averaging the differences over a number of samples. If the offset changes significantly, a notification is sent and the offset history is cleared.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotBrain.FLASH_P0_COMPLETE","title":"FLASH_P0_COMPLETE  <code>instance-attribute</code>","text":"<pre><code>FLASH_P0_COMPLETE = Event[[]]()\n</code></pre> <p>flashing p0 was successful and 'Replica complete' was received</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotBrain.LINE_RECEIVED","title":"LINE_RECEIVED  <code>instance-attribute</code>","text":"<pre><code>LINE_RECEIVED = Event[str]()\n</code></pre> <p>a line has been received from the microcontroller (argument: line as string)</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotHardware","title":"RobotHardware","text":"<pre><code>RobotHardware(\n    modules: list[Module], robot_brain: RobotBrain\n)\n</code></pre> <p>               Bases: <code>Robot</code></p> <p>A robot that consists of hardware modules.</p> <p>It generates Lizard code, forwards output to the hardware modules and sends commands to the robot brain.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.RobotSimulation","title":"RobotSimulation","text":"<pre><code>RobotSimulation(modules: list[Module])\n</code></pre> <p>               Bases: <code>Robot</code></p> <p>A robot that consists of simulated modules.</p> <p>It regularly calls the step method of all modules to allow them to update their internal state.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.SerialCommunication","title":"SerialCommunication","text":"<pre><code>SerialCommunication(\n    *,\n    device_path: str | None = None,\n    baud_rate: int = 115200\n)\n</code></pre> <p>               Bases: <code>Communication</code></p> <p>This module implements a communication via a serial device with a given baud rate.</p> <p>It contains a list of search paths for finding the serial device.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.SerialHardware","title":"SerialHardware","text":"<pre><code>SerialHardware(\n    robot_brain: RobotBrain,\n    *,\n    name: str = \"serial\",\n    rx_pin: int = 26,\n    tx_pin: int = 27,\n    baud: int = 115200,\n    num: int = 1\n)\n</code></pre> <p>               Bases: <code>ModuleHardware</code></p> <p>The serial module represents a serial connection with another device.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WebCommunication","title":"WebCommunication","text":"<pre><code>WebCommunication()\n</code></pre> <p>               Bases: <code>Communication</code></p> <p>Remote connection to the Robot Brain's ESP.</p> <p>This makes it possible to keep developing on your fast computer while communicating with the hardware components connected to a physical Robot Brain.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.Wheels","title":"Wheels","text":"<pre><code>Wheels(**kwargs)\n</code></pre> <p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>This module represents wheels for a two-wheel differential drive.</p> <p>Wheels can be moved using the <code>drive</code> methods and provide measured velocities as an event.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsHardware","title":"WheelsHardware","text":"<pre><code>WheelsHardware(\n    robot_brain: RobotBrain,\n    *,\n    can: CanHardware,\n    name: str = \"wheels\",\n    left_can_address: int = 0,\n    right_can_address: int = 256,\n    m_per_tick: float = 0.01,\n    width: float = 0.5,\n    is_left_reversed: bool = False,\n    is_right_reversed: bool = False\n)\n</code></pre> <p>               Bases: <code>Wheels</code>, <code>ModuleHardware</code></p> <p>This module implements wheels hardware.</p> <p>Drive and stop commands are forwarded to a given Robot Brain. Velocities are read and emitted regularly.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation","title":"WheelsSimulation","text":"<pre><code>WheelsSimulation(width: float = 0.5)\n</code></pre> <p>               Bases: <code>Wheels</code>, <code>ModuleSimulation</code></p> <p>This module simulates two wheels.</p> <p>Drive and stop commands impact internal velocities (linear and angular). A simulated pose is regularly updated with these velocities, while the velocities are emitted as an event.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.angular_velocity","title":"angular_velocity  <code>instance-attribute</code>","text":"<pre><code>angular_velocity: float = 0\n</code></pre> <p>The current angular velocity of the robot.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.friction_factor","title":"friction_factor  <code>instance-attribute</code>","text":"<pre><code>friction_factor: float = 0.0\n</code></pre> <p>The factor of friction for the wheels (0 = no friction, 1 = full friction).</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.inertia_factor","title":"inertia_factor  <code>instance-attribute</code>","text":"<pre><code>inertia_factor: float = 0.0\n</code></pre> <p>The factor of inertia for the wheels (0 = no inertia, 1 = full inertia).</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.is_blocking","title":"is_blocking  <code>instance-attribute</code>","text":"<pre><code>is_blocking: bool = False\n</code></pre> <p>If True, the wheels are blocking and the robot will not move.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.linear_velocity","title":"linear_velocity  <code>instance-attribute</code>","text":"<pre><code>linear_velocity: float = 0\n</code></pre> <p>The current linear velocity of the robot.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.pose","title":"pose  <code>instance-attribute</code>","text":"<pre><code>pose: Pose = Pose(time=time())\n</code></pre> <p>Provides the actual pose of the robot which can alter due to slippage.</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.slip_factor_left","title":"slip_factor_left  <code>instance-attribute</code>","text":"<pre><code>slip_factor_left: float = 0\n</code></pre> <p>The factor of slippage for the left wheel (0 = no slippage, 1 = full slippage).</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.slip_factor_right","title":"slip_factor_right  <code>instance-attribute</code>","text":"<pre><code>slip_factor_right: float = 0\n</code></pre> <p>The factor of slippage for the right wheel (0 = no slippage, 1 = full slippage).</p>"},{"location":"reference/rosys/hardware/#rosys.hardware.WheelsSimulation.width","title":"width  <code>instance-attribute</code>","text":"<pre><code>width: float = width\n</code></pre> <p>The distance between the wheels -- used to calculate actual drift when slip_factor_* is used.</p>"},{"location":"reference/rosys/helpers/","title":"helpers","text":""},{"location":"reference/rosys/helpers/#rosys.helpers.ramp","title":"ramp","text":"<pre><code>ramp(\n    x: float,\n    in1: float,\n    in2: float,\n    out1: float,\n    out2: float,\n    clip: bool = False,\n) -&gt; float\n</code></pre> <p>Map a value x from one range (in1, in2) to another (out1, out2).</p>"},{"location":"reference/rosys/helpers/#rosys.helpers.remove_indentation","title":"remove_indentation","text":"<pre><code>remove_indentation(text: str) -&gt; str\n</code></pre> <p>Remove indentation from a multi-line string based on the indentation of the first line.</p>"},{"location":"reference/rosys/pathplanning/","title":"pathplanning","text":""},{"location":"reference/rosys/pathplanning/#rosys.pathplanning.PathPlanner","title":"PathPlanner","text":"<pre><code>PathPlanner(robot_shape: Prism)\n</code></pre> <p>               Bases: <code>Persistable</code></p> <p>This module runs a path planning algorithm in a separate process.</p> <p>If given, the algorithm respects the given robot shape as well as a dictionary of accessible areas and a dictionary of obstacles, both of which a backed up and restored automatically. The path planner can search paths, check if a spline interferes with obstacles and get the distance of a pose to any obstacle.</p>"},{"location":"reference/rosys/system/","title":"system","text":""},{"location":"reference/rosys/system/#rosys.system.wifi_button","title":"wifi_button","text":"<pre><code>wifi_button()\n</code></pre> <p>               Bases: <code>button</code></p> <p>The WiFi button indicates the current connectivity state and allows setting a new WiFi connection.</p>"},{"location":"reference/rosys/vision/","title":"vision","text":""},{"location":"reference/rosys/vision/#rosys.vision.Autoupload","title":"Autoupload","text":"<p>               Bases: <code>Enum</code></p> <p>Configuration options for image auto-upload behavior to the Learning Loop</p>"},{"location":"reference/rosys/vision/#rosys.vision.Autoupload.ALL","title":"ALL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ALL = 'all'\n</code></pre> <p>Upload mode where every processed image is uploaded</p>"},{"location":"reference/rosys/vision/#rosys.vision.Autoupload.DISABLED","title":"DISABLED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>DISABLED = 'disabled'\n</code></pre> <p>Upload mode where no images are auto-uploaded</p>"},{"location":"reference/rosys/vision/#rosys.vision.Autoupload.FILTERED","title":"FILTERED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>FILTERED = 'filtered'\n</code></pre> <p>Upload mode for images with novel detections within uncertainty thresholds (default)</p>"},{"location":"reference/rosys/vision/#rosys.vision.CameraProjector","title":"CameraProjector","text":"<pre><code>CameraProjector(\n    camera_provider: CalibratableCameraProvider,\n    *,\n    interval: float = 1.0\n)\n</code></pre> <p>The camera projector computes a grid of projected image points on the ground plane.</p> <p>It is mainly used for visualization purposes.</p>"},{"location":"reference/rosys/vision/#rosys.vision.CameraProvider","title":"CameraProvider","text":"<pre><code>CameraProvider()\n</code></pre> <p>               Bases: <code>Generic[T]</code>, <code>Persistable</code></p> <p>A camera provider holds a dictionary of cameras and manages additions and removals.</p> <p>The camera dictionary should not be modified directly but by using the camera provider's methods. This way respective events are emitted and consistency can be taken care of.</p> <p>The camera provider also creates an HTTP route to access camera images.</p>"},{"location":"reference/rosys/vision/#rosys.vision.CameraProvider.CAMERA_ADDED","title":"CAMERA_ADDED  <code>instance-attribute</code>","text":"<pre><code>CAMERA_ADDED = Event[T]()\n</code></pre> <p>a new camera has been added (argument: camera)</p>"},{"location":"reference/rosys/vision/#rosys.vision.CameraProvider.CAMERA_REMOVED","title":"CAMERA_REMOVED  <code>instance-attribute</code>","text":"<pre><code>CAMERA_REMOVED = Event[str]()\n</code></pre> <p>a camera has been removed (argument: camera id)</p>"},{"location":"reference/rosys/vision/#rosys.vision.CameraProvider.NEW_IMAGE","title":"NEW_IMAGE  <code>instance-attribute</code>","text":"<pre><code>NEW_IMAGE = Event[Image]()\n</code></pre> <p>a new image is available from any camera (argument: image)</p>"},{"location":"reference/rosys/vision/#rosys.vision.ConfigurableCamera","title":"ConfigurableCamera","text":"<pre><code>ConfigurableCamera(**kwargs)\n</code></pre> <p>               Bases: <code>Camera</code></p> <p>A generalized interface for adjusting camera parameters like exposure, brightness or fps.</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector","title":"Detector","text":"<pre><code>Detector(*, name: str | None = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>A detector allows detecting objects in images.</p> <p>It also holds an upload queue for sending images with uncertain results to an active learning infrastructure like the Zauberzeug Learning Loop.</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector.NEW_DETECTIONS","title":"NEW_DETECTIONS  <code>instance-attribute</code>","text":"<pre><code>NEW_DETECTIONS = Event[Image]()\n</code></pre> <p>detection on an image is completed (argument: image)</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector.detect","title":"detect  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>detect(\n    image: Image,\n    *,\n    autoupload: Autoupload = Autoupload.FILTERED,\n    tags: list[str] | None = None,\n    source: str | None = None,\n    creation_date: datetime | str | None = None\n) -&gt; Detections | None\n</code></pre> <p>Runs detections on the image and fills the <code>image.detections</code> property.</p> <p>The parameters <code>tags</code>, <code>source</code>, and <code>creation_date</code> are added as metadata if the image is uploaded.</p> <p>Note that the hardware detector uses a lazy strategy to schedule the inference tasks. In particular a queue with a maximum size of 1 is used. This means if the detector is busy, the image is not processed immediately, but queued up. If the <code>detect</code> function is called again, the queued image is dropped and the new image is queued instead. In this case this method returns <code>None</code>.</p> <p>:return: the detections found in the image. :raises DetectorException: if the detection fails.</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector.fetch_detector_info","title":"fetch_detector_info  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>fetch_detector_info() -&gt; DetectorInfo\n</code></pre> <p>Retrieve information about the detector.</p> <p>:return: information about the detector. :raises DetectorException: if the about information cannot be retrieved.</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector.fetch_model_version_info","title":"fetch_model_version_info  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>fetch_model_version_info() -&gt; ModelVersioningInfo\n</code></pre> <p>Retrieve information about the model version and versioning mode.</p> <p>:return: the information about the model versioning as data class. :raises DetectorException: if the detector is not connected or the information cannot be retrieved.</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector.set_model_version","title":"set_model_version  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>set_model_version(\n    version: Literal[\"follow_loop\", \"pause\"] | str,\n) -&gt; None\n</code></pre> <p>Set the model version or versioning mode.</p> <p>Set to \"follow_loop\" to automatically update the model version to the latest version in the learning loop. Set to \"pause\" to stop automatic updates and keep the current model version. Set to a version number (e.g. \"1.2\") to use a specific version.</p> <p>:raises DetectorException: if the version control mode is not valid or the version could not be set.</p>"},{"location":"reference/rosys/vision/#rosys.vision.Detector.upload","title":"upload  <code>abstractmethod</code> <code>async</code>","text":"<pre><code>upload(\n    image: Image,\n    *,\n    tags: list[str] | None = None,\n    source: str | None = None,\n    creation_date: datetime | str | None = None\n) -&gt; None\n</code></pre> <p>Uploads the image to the Learning Loop.</p> <p>The parameters <code>tags</code>, <code>source</code>, and <code>creation_date</code> are added as metadata. If the image has detections, they are also uploaded.</p> <p>:raises DetectorException: if the upload fails.</p>"},{"location":"reference/rosys/vision/#rosys.vision.DetectorHardware","title":"DetectorHardware","text":"<pre><code>DetectorHardware(\n    *,\n    port: int = 8004,\n    name: str | None = None,\n    auto_disconnect: bool = True\n)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>This detector communicates with a YOLO detector via Socket.IO.</p> <p>It automatically connects and reconnects, submits and receives detections and sends images that should be uploaded to the Zauberzeug Learning Loop.</p> <p>Note: Images must be smaller than <code>MAX_IMAGE_SIZE</code> bytes (default: 10 MB).</p>"},{"location":"reference/rosys/vision/#rosys.vision.DetectorHardware.soft_reload","title":"soft_reload  <code>async</code>","text":"<pre><code>soft_reload() -&gt; None\n</code></pre> <p>Trigger a soft reload of the detector.</p> <p>:raises DetectorException: if the communication fails.</p>"},{"location":"reference/rosys/vision/#rosys.vision.DetectorSimulation","title":"DetectorSimulation","text":"<pre><code>DetectorSimulation(\n    camera_provider: CalibratableCameraProvider,\n    *,\n    noise: float = 1.0,\n    detection_delay: float = 0.4,\n    name: str | None = None\n)\n</code></pre> <p>               Bases: <code>Detector</code></p> <p>This detector simulates object detection.</p> <p>It requires a camera provider in order to check visibility using the cameras' calibrations. Individual camera IDs can be added to a set of <code>blocked_cameras</code> to simulate occlusions during pytests. A list of <code>simulated_objects</code> can be filled to define what can be detected. An optional <code>noise</code> parameter controls the spatial accuracy in pixels. An optional <code>detection_delay</code> parameter simulates the time it takes to process an image.</p>"},{"location":"reference/rosys/vision/#rosys.vision.MultiCameraProvider","title":"MultiCameraProvider","text":"<pre><code>MultiCameraProvider(*camera_providers: CameraProvider)\n</code></pre> <p>               Bases: <code>CameraProvider</code></p> <p>A multi-camera provider combines multiple camera providers into one.</p> <p>This is useful if another module requires a single camera provider but the robot has multiple camera sources like USB and WiFi cameras.</p>"},{"location":"reference/rosys/vision/#rosys.vision.RtspCameraProvider","title":"RtspCameraProvider","text":"<pre><code>RtspCameraProvider(\n    *,\n    frame_rate: int = 6,\n    substream: int = 0,\n    jovision_profile: int | None = None,\n    network_interface: str | None = None,\n    auto_scan: bool = True\n)\n</code></pre> <p>               Bases: <code>CameraProvider[RtspCamera]</code></p> <p>This module collects and provides real RTSP streaming cameras.</p>"},{"location":"reference/rosys/vision/#rosys.vision.SimulatedCameraProvider","title":"SimulatedCameraProvider","text":"<pre><code>SimulatedCameraProvider(\n    *,\n    simulate_failing: bool = False,\n    auto_scan: bool = True\n)\n</code></pre> <p>               Bases: <code>CameraProvider[SimulatedCamera]</code></p> <p>This module collects and simulates cameras and generates synthetic images.</p> <p>In the current implementation the images only contain the camera ID and the current time.</p>"},{"location":"reference/rosys/vision/#rosys.vision.SimulatedCameraProvider.scan_for_cameras","title":"scan_for_cameras  <code>async</code>","text":"<pre><code>scan_for_cameras() -&gt; AsyncGenerator[str, Any]\n</code></pre> <p>Simulated device discovery by returning all camera's IDs.</p> <p>If simulate_device_failure is set, disconnected cameras are returned with a fixed probability.</p>"},{"location":"reference/rosys/vision/#rosys.vision.UsbCameraProvider","title":"UsbCameraProvider","text":"<pre><code>UsbCameraProvider(*, auto_scan: bool = True)\n</code></pre> <p>               Bases: <code>CameraProvider[UsbCamera]</code></p> <p>This module collects and provides real USB cameras.</p> <p>Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available.</p>"},{"location":"reference/rosys/vision/#rosys.vision.camera_objects","title":"camera_objects","text":"<pre><code>camera_objects(\n    camera_provider: CalibratableCameraProvider,\n    camera_projector: CameraProjector,\n    *,\n    px_per_m: float = 10000,\n    debug: bool = False,\n    interval: float = 1.0\n)\n</code></pre> <p>               Bases: <code>Group</code></p> <p>This module provides a UI element for displaying cameras in a 3D scene.</p> <p>It requires a camera provider as a source of cameras as well as a camera projector to show the current images projected on the ground plane. The <code>px_per_m</code> argument can be used to scale the camera frustums. With <code>debug=True</code> camera IDs are shown (default: <code>False</code>).</p>"},{"location":"reference/rosys/vision/camera/","title":"camera","text":""},{"location":"reference/rosys/vision/camera/#rosys.vision.camera.ConfigurableCamera","title":"ConfigurableCamera","text":"<pre><code>ConfigurableCamera(**kwargs)\n</code></pre> <p>               Bases: <code>Camera</code></p> <p>A generalized interface for adjusting camera parameters like exposure, brightness or fps.</p>"},{"location":"reference/rosys/vision/rtsp_camera/","title":"rtsp_camera","text":""},{"location":"reference/rosys/vision/rtsp_camera/#rosys.vision.rtsp_camera.RtspCameraProvider","title":"RtspCameraProvider","text":"<pre><code>RtspCameraProvider(\n    *,\n    frame_rate: int = 6,\n    substream: int = 0,\n    jovision_profile: int | None = None,\n    network_interface: str | None = None,\n    auto_scan: bool = True\n)\n</code></pre> <p>               Bases: <code>CameraProvider[RtspCamera]</code></p> <p>This module collects and provides real RTSP streaming cameras.</p>"},{"location":"reference/rosys/vision/simulated_camera/","title":"simulated_camera","text":""},{"location":"reference/rosys/vision/simulated_camera/#rosys.vision.simulated_camera.SimulatedCameraProvider","title":"SimulatedCameraProvider","text":"<pre><code>SimulatedCameraProvider(\n    *,\n    simulate_failing: bool = False,\n    auto_scan: bool = True\n)\n</code></pre> <p>               Bases: <code>CameraProvider[SimulatedCamera]</code></p> <p>This module collects and simulates cameras and generates synthetic images.</p> <p>In the current implementation the images only contain the camera ID and the current time.</p>"},{"location":"reference/rosys/vision/simulated_camera/#rosys.vision.simulated_camera.SimulatedCameraProvider.scan_for_cameras","title":"scan_for_cameras  <code>async</code>","text":"<pre><code>scan_for_cameras() -&gt; AsyncGenerator[str, Any]\n</code></pre> <p>Simulated device discovery by returning all camera's IDs.</p> <p>If simulate_device_failure is set, disconnected cameras are returned with a fixed probability.</p>"},{"location":"reference/rosys/vision/usb_camera/","title":"usb_camera","text":""},{"location":"reference/rosys/vision/usb_camera/#rosys.vision.usb_camera.UsbCameraProvider","title":"UsbCameraProvider","text":"<pre><code>UsbCameraProvider(*, auto_scan: bool = True)\n</code></pre> <p>               Bases: <code>CameraProvider[UsbCamera]</code></p> <p>This module collects and provides real USB cameras.</p> <p>Camera devices are discovered through video4linux (v4l) and accessed with openCV. Therefore the program v4l2ctl and openCV (including python bindings) must be available.</p>"}]}